{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "ADPET68xjlzr",
    "tags": []
   },
   "source": [
    "# 第10章: 事前学習済み言語モデル（GPT型）\n",
    "\n",
    "本章では、GPT型（Transformerのデコーダ型）の事前学習済みモデルを利用して、言語生成、評判分析器（ポジネガ分類器）の構築、ファインチューニング、強化学習などに取り組む。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "dotenv_path = './.env'\n",
    "load_dotenv(dotenv_path)\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `experiments` has been saved to /home/tosshy/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /home/tosshy/.cache/huggingface/token\n",
      "Login successful.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token $HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "C1xKmMckti92",
    "tags": []
   },
   "source": [
    "## 90. 次単語予測\n",
    "\n",
    "“The movie was full of\"に続くトークン（トークン列ではなく一つのトークンであることに注意せよ）として適切なもの上位10個と、その確率（尤度）を求めよ。ただし、言語モデルへのプロンプトがどのようなトークン列に変換されたか、確認せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbdc912478b49bcb3bc1ab882d257b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207c168a5d3a4675bf9bd38738d0e813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af85021560f34ad197a3e2e6b0264f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa300c517f204c1796e14282c35eb6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd42434cfbd34c48982f579219dac023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c562ef7f8b472fac1ab8c4d093158a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B-Instruct')\n",
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-1B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Ġmovie', 'Ġwas', 'Ġfull', 'Ġof']\n",
      "[128000, 791, 5818, 574, 2539, 315]\n",
      "<|begin_of_text|>The movie was full of\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The movie was full of'\n",
    "\n",
    "encoded = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "tokenized = tokenizer.tokenize(sentence)\n",
    "print(tokenized)\n",
    "\n",
    "input_ids = tokenizer(sentence).input_ids\n",
    "print(input_ids)\n",
    "\n",
    "decoded = tokenizer.decode(input_ids)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,    791,   5818,    574,   2539,    315]], device='cuda:1'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:1')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 実験：各系列における尤度が最大のものを選んだ結果\n",
    "model.to(device)\n",
    "encoded.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,    791,   5818,    574,   2539,    315]], device='cuda:1'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:1')}\n",
      "Next token id: 1957\n",
      "Next token:  action\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "encoded.to(device)\n",
    "\n",
    "print(encoded)\n",
    "\n",
    "outputs = model(**encoded)\n",
    "logits = outputs.logits # {batch_size, seq_len, vocab_size}\n",
    "# logits[:, k, :]はinput_ids[k-1]までを使って計算されたスコア\n",
    "next_token_logits = logits[:, -1, :]\n",
    "next_token_id = next_token_logits.argmax(-1).item()\n",
    "print(f'Next token id: {next_token_id}')\n",
    "next_token = tokenizer.decode([next_token_id])\n",
    "print(f'Next token: {next_token}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 next tokens\n",
      "1. Token:  action, Probability: 0.0852\n",
      "2. Token:  suspense, Probability: 0.0344\n",
      "3. Token:  drama, Probability: 0.0242\n",
      "4. Token:  excitement, Probability: 0.0190\n",
      "5. Token:  great, Probability: 0.0189\n",
      "6. Token:  surprises, Probability: 0.0155\n",
      "7. Token:  interesting, Probability: 0.0143\n",
      "8. Token:  twists, Probability: 0.0136\n",
      "9. Token:  exciting, Probability: 0.0132\n",
      "10. Token:  memorable, Probability: 0.0099\n"
     ]
    }
   ],
   "source": [
    "top_k = 10\n",
    "next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "top_k_probs, top_k_indices = torch.topk(next_token_probs, top_k) # (batch_size, top_k)\n",
    "\n",
    "print(f'Top {top_k} next tokens')\n",
    "for i in range(top_k):\n",
    "    token_id = top_k_indices[0, i].item()\n",
    "    token = tokenizer.decode([token_id])\n",
    "    prob = top_k_probs[0, i].item()\n",
    "    print(f'{i+1}. Token: {token}, Probability: {prob:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "s1RhOldA0meh",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 91. 続きのテキストの予測\n",
    "\n",
    "“The movie was full of\"に続くテキストを複数予測せよ。このとき、デコーディングの方法や温度パラメータ（temperature）を変えながら、予測される複数のテキストの変化を観察せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'The movie was full of'\n",
    "encoded = tokenizer(prompt, return_tensors='pt')\n",
    "input_ids = encoded.input_ids.to(device)\n",
    "attention_mask = encoded.attention_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Default (Greedy-like) ---\n",
      "The movie was full of memorable characters, exciting plot twists, and stunning visuals. But, what made it truly unforgettable was the emotional resonance it brought to the audience.\n",
      "\n",
      "For many viewers, the movie was a powerful reminder of the human experience. It\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Greedy Searchに近い\n",
    "print('--- Default (Greedy-like) ---')\n",
    "outputs_default = model.generate(input_ids, attention_mask=attention_mask, max_length=50, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs_default[0], skip_special_tokens=True))\n",
    "print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sampling with Temperature (0.7) ---\n",
      "The movie was full of memorable moments, but one scene that really stands out to me is the infamous \"I'm a little tea pot\" line from the character, Willy Wonka.\n",
      "\n",
      "I remember being so mesmerized by the way Charlie Bucket\n",
      "------------------------------\n",
      "--- Sampling with Temperature (1.5) ---\n",
      "The movie was full of exciting moments but, in the final cut, only these few scenes were retained.\n",
      "The scenes that were omitted are the first shot of an empty theater, where it is left to believe that it has stood vacant.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# サンプリング (do_sample=True) と Temperature\n",
    "# Temperature < 1.0 : より決定的\n",
    "# Temperature > 1.0 : よりランダム\n",
    "print('--- Sampling with Temperature (0.7) ---')\n",
    "outputs_temp_low = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(outputs_temp_low[0], skip_special_tokens=True))\n",
    "print('-' * 30)\n",
    "\n",
    "print('--- Sampling with Temperature (1.5) ---')\n",
    "outputs_temp_high = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    temperature=1.5,\n",
    "    top_k=50,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(outputs_temp_high[0], skip_special_tokens=True))\n",
    "print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top-k Sampling (k=30) ---\n",
      "The movie was full of action and suspense, but it was the way the characters interacted with each other that made it truly unforgettable.\n",
      "I remember the movie like it was yesterday. I was a teenager at the time, and I had seen the\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Top-k サンプリング\n",
    "# 次のトークンを予測する際に，確率上位k個の中からサンプリングする\n",
    "print('--- Top-k Sampling (k=30) ---')\n",
    "outputs_top_k = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    top_k=30,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(outputs_top_k[0], skip_special_tokens=True))\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top-p (Nucleus) Sampling (p=0.9) ---\n",
      "The movie was full of surprises, but one of the most memorable moments was when the main character, a young girl named Lily, found out that her mother was a famous actress. She had always known that her mother was famous, but she had\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Top-p (Nucleus) サンプリング\n",
    "# 確率の累積がpを超える最小のトークンセットからサンプリングする\n",
    "print('--- Top-p (Nucleus) Sampling (p=0.9) ---')\n",
    "outputs_top_p = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    top_k=0, # top_kとtop_pは通常どちらか一方を指定するか、top_k=0でtop_pを有効にする\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(outputs_top_p[0], skip_special_tokens=True))\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Beam Search (num_beams=5) ---\n",
      "The movie was full of action, suspense, and romance. The plot was engaging, and the characters were well-developed and relatable. The special effects were impressive, and the cinematography was stunning. Overall, the movie was a thrilling ride\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 5. ビームサーチ\n",
    "# 複数の候補 (ビーム) を保持しながら探索する\n",
    "print(\"--- Beam Search (num_beams=5) ---\")\n",
    "outputs_beam = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    early_stopping=True, # EOSトークンが出たら早めに打ち切る\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(outputs_beam[0], skip_special_tokens=True))\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Beam Search with num_return_sequences=3 ---\n",
      "Output 1: The movie was full of action, suspense, and romance. The plot was engaging, and the characters were well-developed and relatable. The special effects were impressive, and the cinematography was stunning. Overall, the movie was a thrilling ride\n",
      "Output 2: The movie was full of action, suspense, and romance. The plot was engaging, and the characters were well-developed and relatable. The special effects were impressive, and the cinematography was stunning. Overall, I would highly recommend this movie\n",
      "Output 3: The movie was full of action, suspense, and romance. The plot was engaging, and the characters were well-developed and relatable. The special effects were impressive, and the cinematography was stunning. Overall, the movie was a thrilling and\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 複数の異なる出力を得るために num_return_sequences を使用\n",
    "print(\"--- Beam Search with num_return_sequences=3 ---\")\n",
    "outputs_beam_multiple = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    num_return_sequences=3,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "for i, output in enumerate(outputs_beam_multiple):\n",
    "    print(f\"Output {i+1}: {tokenizer.decode(output, skip_special_tokens=True)}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "7ZFadg6B8VdA",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 92. 予測されたテキストの確率を計算\n",
    "\n",
    "“The movie was full of\"に続くテキストを予測し、生成された各単語の尤度を表示せよ（生成されるテキストが長いと出力が読みにくくなるので、適当な長さで生成を打ち切るとよい）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'The movie was full of'\n",
    "encoded = tokenizer(prompt, return_tensors='pt')\n",
    "input_ids = encoded.input_ids.to(device)\n",
    "attention_mask = encoded.attention_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input prompt: The movie was full of\n",
      "Generated tokens and their probabilities:\n",
      "Token: ' action', Probability: 0.5336\n",
      "Token: ',', Probability: 0.5750\n",
      "Token: ' suspense', Probability: 0.8670\n",
      "Token: ',', Probability: 1.0000\n",
      "Token: ' and', Probability: 1.0000\n",
      "Token: ' romance', Probability: 0.6102\n",
      "Token: ',', Probability: 0.3331\n",
      "Token: ' but', Probability: 0.3680\n",
      "Token: ' it', Probability: 0.4549\n",
      "Token: ' was', Probability: 0.6033\n",
      "Token: ' also', Probability: 0.7383\n",
      "Token: ' a', Probability: 0.8530\n",
      "Token: ' thought', Probability: 0.0197\n",
      "Token: '-pro', Probability: 1.0000\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids, \n",
    "    attention_mask=attention_mask, \n",
    "    max_length=20, \n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True\n",
    ")\n",
    "\n",
    "sequences = list(outputs.sequences.squeeze(0))\n",
    "scores = outputs.scores\n",
    "\n",
    "prompt_len = input_ids.shape[1]\n",
    "\n",
    "print(f\"Input prompt: {tokenizer.decode(sequences[:prompt_len], skip_special_tokens=True)}\")\n",
    "print(\"Generated tokens and their probabilities:\")\n",
    "\n",
    "for k in range(len(scores)):\n",
    "    current_token_idx_in_sequence = prompt_len + k\n",
    "\n",
    "    generated_token_id = sequences[current_token_idx_in_sequence]\n",
    "\n",
    "    generated_token_str = tokenizer.decode([generated_token_id])\n",
    "\n",
    "    step_logits = scores[k]\n",
    "\n",
    "    step_probs = torch.softmax(step_logits, dim=-1).squeeze()\n",
    "\n",
    "    prob_of_generated_token = step_probs[generated_token_id].item()\n",
    "\n",
    "    print(f\"Token: '{generated_token_str}', Probability: {prob_of_generated_token:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "FvNCTMj6OegF",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 93. パープレキシティ\n",
    "\n",
    "適当な文を準備して、事前学習済み言語モデルでパープレキシティを測定せよ。例えば、\n",
    "\n",
    "+ The movie was full of surprises\n",
    "+ The movies were full of surprises\n",
    "+ The movie were full of surprises\n",
    "+ The movies was full of surprises\n",
    "\n",
    "の4文に対して、パープレキシティを測定して観察せよ（最後の2つの文は故意に文法的な間違いを入れた）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'The movie was full of surprises',\n",
    "    'The movies were full of surprises',\n",
    "    'The movie were full of surprises',\n",
    "    'The movies was full of surprises'\n",
    "]\n",
    "\n",
    "encoded = tokenizer(sentences, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,    791,   5818,    574,   2539,    315,  46540],\n",
       "        [128000,    791,   9698,   1051,   2539,    315,  46540],\n",
       "        [128000,    791,   5818,   1051,   2539,    315,  46540],\n",
       "        [128000,    791,   9698,    574,   2539,    315,  46540]],\n",
       "       device='cuda:1'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1]], device='cuda:1')}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "encoded.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The movie was full of surprises, Perplexity: 159.2811737060547\n",
      "Sentence: The movies were full of surprises, Perplexity: 265.9172668457031\n",
      "Sentence: The movie were full of surprises, Perplexity: 460.8365173339844\n",
      "Sentence: The movies was full of surprises, Perplexity: 407.7308349609375\n"
     ]
    }
   ],
   "source": [
    "input_ids = encoded.input_ids\n",
    "outputs = model(**encoded)\n",
    "logits = outputs.logits\n",
    "\n",
    "for i in range(logits.shape[0]):\n",
    "    current_prediction_logits = logits[i, :-1, :]\n",
    "    current_target_ids = input_ids[i, 1:]\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    mean_neg_log_likelihood = criterion(current_prediction_logits, current_target_ids)\n",
    "\n",
    "    ppl = torch.exp(mean_neg_log_likelihood)\n",
    "    print(f'Sentence: {sentences[i]}, Perplexity: {ppl.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-7fB-n9suYg"
   },
   "source": [
    "## 94. チャットテンプレート\n",
    "\n",
    "\"What do you call a sweet eaten after dinner?\"という問いかけに対する応答を生成するため、チャットテンプレートを適用し、言語モデルに与えるべきプロンプトを作成せよ。また、そのプロンプトに対する応答を生成し、表示せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated prompt:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 Jun 2025\n",
      "\n",
      "Answer the following question.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What do you call a sweet eaten after dinner?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "instruction = 'Answer the following question.'\n",
    "text = 'What do you call a sweet eaten after dinner?'\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": instruction},\n",
    "    {\"role\": \"user\", \"content\": text}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print('Generated prompt:')\n",
    "print(prompt)\n",
    "print('=' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      "I don't know what you're asking, but I can try to help. Is the answer \"dessert\"?\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "if 'token_type_ids' in inputs:\n",
    "    inputs.pop('token_type_ids')\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print('Model response:')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "PT-bk0XWIZ2E",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 95. マルチターンのチャット\n",
    "\n",
    "問題94で生成された応答に対して、追加で\"Please give me the plural form of the word with its spelling in reverse order.\"と問いかけたときの応答を生成・表示せよ。また、その時に言語モデルに与えるプロンプトを確認せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-turn chat prompt\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 Jun 2025\n",
      "\n",
      "Answer the following question.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What do you call a sweet eaten after dinner?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I don't know what you're asking, but I can try to help. Is the answer \"dessert\"?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Please give me the plural form of the word with its spelling in reverse order.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "previous_response = response\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": instruction},\n",
    "    {\"role\": \"user\", \"content\": \"What do you call a sweet eaten after dinner?\"},\n",
    "    {\"role\": \"assistant\", \"content\": previous_response},\n",
    "    {\"role\": \"user\", \"content\": \"Please give me the plural form of the word with its spelling in reverse order.\"}\n",
    "]\n",
    "\n",
    "multi_turn_prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print('Multi-turn chat prompt')\n",
    "print(multi_turn_prompt)\n",
    "print('=' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-turn response:\n",
      "The word \"dessert\" spelled in reverse order is \"trseSSED\".\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(multi_turn_prompt, return_tensors='pt')\n",
    "if 'token_type_ids' in inputs:\n",
    "    inputs.pop('token_type_ids')\n",
    "inputs = inputs.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "multi_turn_response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print('Multi-turn response:')\n",
    "print(multi_turn_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "qH0YortL0afd",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 96. プロンプトによる感情分析\n",
    "\n",
    "事前学習済み言語モデルで感情分析を行いたい。テキストを含むプロンプトを事前学習済み言語モデルに与え、（ファインチューニングは行わずに）テキストのポジネガを予測するという戦略で、[SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip)の開発データにおける正解率を測定せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('llm-jp/llm-jp-3-150m-instruct3')\n",
    "model = AutoModelForCausalLM.from_pretrained('llm-jp/llm-jp-3-150m-instruct3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67344</th>\n",
       "      <td>a delightful comedy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67345</th>\n",
       "      <td>anguish , anger and frustration</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67346</th>\n",
       "      <td>at achieving the modest , crowd-pleasing goals...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67347</th>\n",
       "      <td>a patient viewer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67348</th>\n",
       "      <td>this new jangle of noise , mayhem and stupidit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67349 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label\n",
       "0           hide new secretions from the parental units       0\n",
       "1                   contains no wit , only labored gags       0\n",
       "2      that loves its characters and communicates som...      1\n",
       "3      remains utterly satisfied to remain the same t...      0\n",
       "4      on the worst revenge-of-the-nerds clichés the ...      0\n",
       "...                                                  ...    ...\n",
       "67344                               a delightful comedy       1\n",
       "67345                   anguish , anger and frustration       0\n",
       "67346  at achieving the modest , crowd-pleasing goals...      1\n",
       "67347                                  a patient viewer       1\n",
       "67348  this new jangle of noise , mayhem and stupidit...      0\n",
       "\n",
       "[67349 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_path = './data/SST-2/train.tsv'\n",
    "dev_path = './data/SST-2/dev.tsv'\n",
    "\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "dev_df = pd.read_csv(dev_path, sep='\\t')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences:   0%|                                                                                                                                                                                | 0/872 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sentences: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 872/872 [02:03<00:00,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 872\n",
      "Correct predictions: 0\n",
      "Unable to predict: 872\n",
      "Wrong predictions: 0\n",
      "Accuracy: 0.0000 (0/872)\n",
      "Unable to predict rate: 1.0000 (872/872)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "dev_sentences = dev_df.sentence\n",
    "dev_labels = dev_df.label\n",
    "\n",
    "predictions = []\n",
    "\n",
    "instruction = '以下の文がポジティブかネガティブのどちらなのか判定してください．\"ポジティブ\"か\"ネガティブ\"で回答してください．'\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for i in tqdm(range(len(dev_sentences)), desc='Processing sentences'):\n",
    "    sentence = dev_sentences.iloc[i]\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": sentence}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    if 'token_type_ids' in inputs:\n",
    "        inputs.pop('token_type_ids')\n",
    "    inputs = inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=15,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    predictions.append(response.strip())\n",
    "\n",
    "correct = 0\n",
    "unable_to_predict = 0\n",
    "for i, pred in enumerate(predictions):\n",
    "    true_label = dev_labels.iloc[i]\n",
    "    pred_lower = pred.lower()\n",
    "\n",
    "    if true_label == 1 and 'ポジティブ' in pred_lower:\n",
    "        correct += 1\n",
    "    elif true_label == 0 and 'ネガティブ' in pred_lower:\n",
    "        correct += 1\n",
    "    elif 'ポジティブ' not in pred_lower and 'ネガティブ' not in pred_lower:\n",
    "        unable_to_predict += 1\n",
    "\n",
    "total = len(predictions)\n",
    "accuracy = correct / total\n",
    "unable_rate = unable_to_predict / total\n",
    "\n",
    "print(f'Total samples: {total}')\n",
    "print(f'Correct predictions: {correct}')\n",
    "print(f'Unable to predict: {unable_to_predict}')\n",
    "print(f'Wrong predictions: {total - correct - unable_to_predict}')\n",
    "print(f'Accuracy: {accuracy:.4f} ({correct}/{total})')\n",
    "print(f'Unable to predict rate: {unable_rate:.4f} ({unable_to_predict}/{total})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Examples of unable to predict:\n",
      "  Sentence: \"it 's a charming and often affecting journey . \"\n",
      "  Prediction: \"「it 's a charming and often affecting journey .」は、直訳すると\"\n",
      "  True label: 1\n",
      "--------------------------------------------------\n",
      "  Sentence: \"unflinchingly bleak and desperate \"\n",
      "  Prediction: \"「unflinchingly bleak and desperate」は、直訳すると「絶望\"\n",
      "  True label: 0\n",
      "--------------------------------------------------\n",
      "  Sentence: \"allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . \"\n",
      "  Prediction: \"Among the many reasons why a filmmaker might be poised to embar\"\n",
      "  True label: 1\n",
      "--------------------------------------------------\n",
      "  Sentence: \"the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales . \"\n",
      "  Prediction: \"質問内容は個人のプライバシーを尊重し、個人情報保護の観点から\"\n",
      "  True label: 1\n",
      "--------------------------------------------------\n",
      "  Sentence: \"it 's slow -- very , very slow . \"\n",
      "  Prediction: \"「it 's slow -- very , very slow .」は、英語の\"\n",
      "  True label: 0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if unable_to_predict > 0:\n",
    "    print(f'\\nExamples of unable to predict:')\n",
    "    count = 0\n",
    "    for i, pred in enumerate(predictions):\n",
    "        pred_lower = pred.lower()\n",
    "        if 'ポジティブ' not in pred_lower and 'ネガティブ' not in pred_lower:\n",
    "            print(f'  Sentence: \"{dev_sentences.iloc[i]}\"')\n",
    "            print(f'  Prediction: \"{pred}\"')\n",
    "            print(f'  True label: {dev_labels.iloc[i]}')\n",
    "            print('-' * 50)\n",
    "            count += 1\n",
    "            if count >= 5:  # 最初の5件だけ表示\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "giA6FivrKaSf",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 97. 埋め込みに基づく感情分析\n",
    "\n",
    "事前学習済み言語モデルでテキストをベクトルで表現（エンコード）し、そのベクトルにフィードフォワード層を通すことで極性ラベルを予測するモデルを学習せよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama3.2-3B-InstuructをQLoRAでチューニングした場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, LlamaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67344</th>\n",
       "      <td>a delightful comedy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67345</th>\n",
       "      <td>anguish , anger and frustration</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67346</th>\n",
       "      <td>at achieving the modest , crowd-pleasing goals...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67347</th>\n",
       "      <td>a patient viewer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67348</th>\n",
       "      <td>this new jangle of noise , mayhem and stupidit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67349 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label\n",
       "0           hide new secretions from the parental units       0\n",
       "1                   contains no wit , only labored gags       0\n",
       "2      that loves its characters and communicates som...      1\n",
       "3      remains utterly satisfied to remain the same t...      0\n",
       "4      on the worst revenge-of-the-nerds clichés the ...      0\n",
       "...                                                  ...    ...\n",
       "67344                               a delightful comedy       1\n",
       "67345                   anguish , anger and frustration       0\n",
       "67346  at achieving the modest , crowd-pleasing goals...      1\n",
       "67347                                  a patient viewer       1\n",
       "67348  this new jangle of noise , mayhem and stupidit...      0\n",
       "\n",
       "[67349 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_path = './data/SST-2/train.tsv'\n",
    "dev_path = './data/SST-2/dev.tsv'\n",
    "\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "dev_df = pd.read_csv(dev_path, sep='\\t')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaBinaryClassifier(nn.Module):\n",
    "    def __init__(self, llama_model, hidden_size):\n",
    "        super().__init__()\n",
    "        self.llama_model = llama_model\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        for param in self.llama_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.llama_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            use_cache=False\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            masked_hidden = hidden_states.clone()\n",
    "            masked_hidden[attention_mask == 0] = float('-inf')\n",
    "            pooled = torch.max(masked_hidden, dim=1)[0]\n",
    "        else:\n",
    "            pooled = torch.max(hidden_states, dim=1)[0]\n",
    "\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SST2Dataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=256):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = str(self.sentences.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding.input_ids.flatten(),\n",
    "            'attention_mask': encoding.attention_mask.flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        print(f\"Available VRAM: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "        print(f\"Cached VRAM: {torch.cuda.memory_reserved() / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA RTX A4500\n",
      "Total VRAM: 21.0 GB\n",
      "Available VRAM: 0.0 GB\n",
      "Cached VRAM: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and base LlamaModel...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eed8f3be3d84939ae9b8573e57cda6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Loading tokenizer and base LlamaModel...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-3B-Instruct')\n",
    "model = LlamaModel.from_pretrained(\n",
    "    'meta-llama/Llama-3.2-3B-Instruct',\n",
    "    device_map={\"\": 1},\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:\n",
      "layers.0.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.0.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.0.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.0.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.0.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.0.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.0.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.12.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.12.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.12.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.12.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.12.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.12.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.12.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.13.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.13.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.13.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.13.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.13.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.13.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.13.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.14.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.14.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.14.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.14.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.14.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.14.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.14.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.15.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.15.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.15.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.15.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.15.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.15.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.15.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.16.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.16.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.16.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.16.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.16.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.16.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.16.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.17.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.17.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.17.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.17.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.17.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.17.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.17.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.18.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.18.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.18.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.18.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.18.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.18.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.18.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.19.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.19.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.19.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.19.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.19.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.19.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.19.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.20.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.20.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.20.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.20.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.20.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.20.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.20.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.21.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.21.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.21.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.21.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.21.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.21.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.21.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.22.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.22.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.22.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.22.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.22.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.22.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.22.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.23.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.23.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.23.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.23.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.23.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.23.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.23.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.24.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.24.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.24.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.24.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.24.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.24.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.24.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.25.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.25.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.25.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.25.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.25.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.25.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.25.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.26.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.26.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.26.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.26.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.26.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.26.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.26.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.27.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.27.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.27.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.27.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.27.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.27.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.27.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "\n",
      "Model attributes:\n",
      "['_convert_head_mask_to_5d', '_copy_lm_head_original_to_resized', '_get_resized_lm_head', '_init_added_lm_head_bias_with_mean', '_init_added_lm_head_weights_with_mean', 'get_head_mask', 'prune_heads']\n",
      "\n",
      "Model config: LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model structure:\")\n",
    "for name, module in model.named_modules():\n",
    "    if 'head' in name.lower() or 'output' in name.lower() or 'proj' in name.lower():\n",
    "        print(f\"{name}: {type(module)}\")\n",
    "\n",
    "print(\"\\nModel attributes:\")\n",
    "print([attr for attr in dir(model) if 'head' in attr.lower()])\n",
    "\n",
    "print(f\"\\nModel config: {model.config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up padding token...\n",
      "Set pad_token to: <|eot_id|>\n",
      "Set model pad_token_id to: 128009\n",
      "tokenizer.pad_token: <|eot_id|>\n",
      "tokenizer.pad_token_id: 128009\n",
      "tokenizer.eos_token: <|eot_id|>\n",
      "tokenizer.eos_token_id: 128009\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up padding token...\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Set pad_token to: {tokenizer.pad_token}\")\n",
    "\n",
    "if hasattr(model, 'config'):\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    print(f\"Set model pad_token_id to: {model.config.pad_token_id}\")\n",
    "\n",
    "print(f\"tokenizer.pad_token: {tokenizer.pad_token}\")\n",
    "print(f\"tokenizer.pad_token_id: {tokenizer.pad_token_id}\")\n",
    "print(f\"tokenizer.eos_token: {tokenizer.eos_token}\")\n",
    "print(f\"tokenizer.eos_token_id: {tokenizer.eos_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden size: 3072\n"
     ]
    }
   ],
   "source": [
    "hidden_size = model.config.hidden_size\n",
    "print(f'Hidden size: {hidden_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_4bit(W: torch.Tensor):\n",
    "    # per-row scale\n",
    "    maxv = W.abs().amax(dim=1, keepdim=True) # (out, 1)\n",
    "    scale = maxv / 7.0\n",
    "    Wq = torch.clamp((W / scale).round(), -8, 7).to(torch.int8)\n",
    "    return Wq, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantLinear(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear):\n",
    "        super().__init__()\n",
    "        W = linear.weight.data # (out, in)\n",
    "        Wq, scale = quantize_4bit(W)\n",
    "        self.register_buffer('Wq', Wq)\n",
    "        self.register_buffer('scale', scale)\n",
    "        if linear.bias is not None:\n",
    "            self.bias = nn.Parameter(linear.bias.data.clone())\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # dequantize on the fly\n",
    "        W = self.Wq.float() * self.scale\n",
    "        return F.linear(x, W, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, r=8, alpha=16):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "        self.scaling = alpha / r\n",
    "        self.down = nn.Linear(in_features, r, bias=False)\n",
    "        self.up   = nn.Linear(r, out_features, bias=False)\n",
    "        nn.init.kaiming_uniform_(self.down.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.up.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.up(self.down(x)) * self.scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantLoRALinear(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear, r=8, alpha=16):\n",
    "        super().__init__()\n",
    "        self.quant = QuantLinear(linear)\n",
    "        self.lora = LoRALinear(linear.in_features, linear.out_features, r=r, alpha=alpha)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.quant(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_qlora(\n",
    "        model, \n",
    "        target_substrings=(\n",
    "            # Attention\n",
    "            'q_proj','v_proj','k_proj','o_proj'\n",
    "            # MLP (FFN)\n",
    "            # 'gate_proj', 'down_proj', 'up_proj'\n",
    "        )\n",
    "    ):\n",
    "    for name, module in list(model.named_modules()):\n",
    "        if isinstance(module, nn.Linear) and any(s in name for s in target_substrings):\n",
    "            parent_name, attr = name.rsplit('.', 1)\n",
    "            parent_mod = dict(model.named_modules())[parent_name]\n",
    "            setattr(parent_mod,\n",
    "                    attr,\n",
    "                    QuantLoRALinear(module, r=8, alpha=16)\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = LlamaBinaryClassifier(model, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in classifier_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "apply_qlora(classifier_model)\n",
    "for p in classifier_model.modules():\n",
    "    if isinstance(p, LoRALinear):\n",
    "        for sub in (p.down, p.up):\n",
    "            for q in sub.parameters():\n",
    "                q.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "batch_size = 4\n",
    "\n",
    "train_dataset = SST2Dataset(train_df.sentence, train_df.label, tokenizer, max_length)\n",
    "dev_dataset = SST2Dataset(dev_df.sentence, dev_df.label, tokenizer, max_length)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=16,\n",
    "    pin_memory=True\n",
    ")\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=16,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "\n",
    "lora_params = [p for p in classifier_model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(lora_params, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA RTX A4500\n",
      "Total VRAM: 21.0 GB\n",
      "Available VRAM: 0.0 GB\n",
      "Cached VRAM: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "classifier_model, optimizer, train_loader, dev_loader = accelerator.prepare(\n",
    "    classifier_model, optimizer, train_loader, dev_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaBinaryClassifier(\n",
       "  (llama_model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): QuantLoRALinear(\n",
       "            (quant): QuantLinear()\n",
       "            (lora): LoRALinear(\n",
       "              (down): Linear(in_features=3072, out_features=8, bias=False)\n",
       "              (up): Linear(in_features=8, out_features=3072, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (k_proj): QuantLoRALinear(\n",
       "            (quant): QuantLinear()\n",
       "            (lora): LoRALinear(\n",
       "              (down): Linear(in_features=3072, out_features=8, bias=False)\n",
       "              (up): Linear(in_features=8, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (v_proj): QuantLoRALinear(\n",
       "            (quant): QuantLinear()\n",
       "            (lora): LoRALinear(\n",
       "              (down): Linear(in_features=3072, out_features=8, bias=False)\n",
       "              (up): Linear(in_features=8, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (o_proj): QuantLoRALinear(\n",
       "            (quant): QuantLinear()\n",
       "            (lora): LoRALinear(\n",
       "              (down): Linear(in_features=3072, out_features=8, bias=False)\n",
       "              (up): Linear(in_features=8, out_features=3072, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (classifier): Linear(in_features=3072, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "classifier_model.to(accelerator.device)\n",
    "classifier_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|███████████████████████████████████████████████████████████████| 16838/16838 [5:19:56<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.1552\n",
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        with accelerator.autocast():\n",
    "            logits = classifier_model(input_ids, attention_mask)\n",
    "            loss = criterion(logits.squeeze(-1), labels)\n",
    "        \n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 218/218 [02:07<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development Set Accuracy: 0.9633\n",
      "GPU: NVIDIA RTX A4500\n",
      "Total VRAM: 21.0 GB\n",
      "Available VRAM: 10.8 GB\n",
      "Cached VRAM: 19.8 GB\n",
      "\n",
      "Prediction examples:\n",
      "Sentence: it 's a charming and often affecting journey . \n",
      "True label: 1 (Positive)\n",
      "Predicted: 1 (Positive)\n",
      "Correct: True\n",
      "--------------------------------------------------\n",
      "Sentence: unflinchingly bleak and desperate \n",
      "True label: 0 (Negative)\n",
      "Predicted: 0 (Negative)\n",
      "Correct: True\n",
      "--------------------------------------------------\n",
      "Sentence: allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . \n",
      "True label: 1 (Positive)\n",
      "Predicted: 1 (Positive)\n",
      "Correct: True\n",
      "--------------------------------------------------\n",
      "Sentence: the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales . \n",
      "True label: 1 (Positive)\n",
      "Predicted: 1 (Positive)\n",
      "Correct: True\n",
      "--------------------------------------------------\n",
      "Sentence: it 's slow -- very , very slow . \n",
      "True label: 0 (Negative)\n",
      "Predicted: 0 (Negative)\n",
      "Correct: True\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 評価\n",
    "classifier_model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dev_loader, desc='Evaluating'):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        logits = classifier_model(input_ids, attention_mask)\n",
    "        predictions = torch.sigmoid(logits.squeeze(-1)) > 0.5\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 精度の計算\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f'Development Set Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# メモリ使用量の最終チェック\n",
    "check_gpu_memory()\n",
    "\n",
    "# いくつかの予測例を表示\n",
    "print(\"\\nPrediction examples:\")\n",
    "for i in range(5):\n",
    "    sentence = dev_df['sentence'].iloc[i]\n",
    "    true_label = dev_df['label'].iloc[i]\n",
    "    pred_label = int(all_predictions[i])\n",
    "    \n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"True label: {true_label} ({'Positive' if true_label == 1 else 'Negative'})\")\n",
    "    print(f\"Predicted: {pred_label} ({'Positive' if pred_label == 1 else 'Negative'})\")\n",
    "    print(f\"Correct: {true_label == pred_label}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llm-jp-3-150m-instruct3をフルファインチューニングした場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_path = './data/SST-2/train.tsv'\n",
    "dev_path = './data/SST-2/dev.tsv'\n",
    "\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "dev_df = pd.read_csv(dev_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlmjpBinaryClassifier(nn.Module):\n",
    "    def __init__(self, llmjp_model, hidden_size):\n",
    "        super().__init__()\n",
    "        self.llmjp_model = llmjp_model\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        for param in self.llmjp_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.llmjp_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            use_cache=False\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            masked_hidden = hidden_states.clone()\n",
    "            masked_hidden[attention_mask == 0] = float('-inf')\n",
    "            pooled = torch.max(masked_hidden, dim=1)[0]\n",
    "        else:\n",
    "            pooled = torch.max(hidden_states, dim=1)[0]\n",
    "\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SST2Dataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=256):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = str(self.sentences.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding.input_ids.flatten(),\n",
    "            'attention_mask': encoding.attention_mask.flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and base LLMJPModel...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer and base LLMJPModel...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('llm-jp/llm-jp-3-150m-instruct3')\n",
    "model = AutoModel.from_pretrained(\n",
    "    'llm-jp/llm-jp-3-150m-instruct3',\n",
    "    device_map={\"\": 1},\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:\n",
      "layers.0.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.0.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.0.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.0.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.0.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.0.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.0.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "\n",
      "Model attributes:\n",
      "['_convert_head_mask_to_5d', '_copy_lm_head_original_to_resized', '_get_resized_lm_head', '_init_added_lm_head_bias_with_mean', '_init_added_lm_head_weights_with_mean', 'get_head_mask', 'prune_heads']\n",
      "\n",
      "Model config: LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 99584\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model structure:\")\n",
    "for name, module in model.named_modules():\n",
    "    if 'head' in name.lower() or 'output' in name.lower() or 'proj' in name.lower():\n",
    "        print(f\"{name}: {type(module)}\")\n",
    "\n",
    "print(\"\\nModel attributes:\")\n",
    "print([attr for attr in dir(model) if 'head' in attr.lower()])\n",
    "\n",
    "print(f\"\\nModel config: {model.config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up padding token...\n",
      "Set model pad_token_id to: 4\n",
      "tokenizer.pad_token: <PAD|LLM-jp>\n",
      "tokenizer.pad_token_id: 4\n",
      "tokenizer.eos_token: </s>\n",
      "tokenizer.eos_token_id: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up padding token...\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Set pad_token to: {tokenizer.pad_token}\")\n",
    "\n",
    "if hasattr(model, 'config'):\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    print(f\"Set model pad_token_id to: {model.config.pad_token_id}\")\n",
    "\n",
    "print(f\"tokenizer.pad_token: {tokenizer.pad_token}\")\n",
    "print(f\"tokenizer.pad_token_id: {tokenizer.pad_token_id}\")\n",
    "print(f\"tokenizer.eos_token: {tokenizer.eos_token}\")\n",
    "print(f\"tokenizer.eos_token_id: {tokenizer.eos_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden size: 512\n"
     ]
    }
   ],
   "source": [
    "hidden_size = model.config.hidden_size\n",
    "print(f'Hidden size: {hidden_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = LlmjpBinaryClassifier(model, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "batch_size = 4\n",
    "\n",
    "train_dataset = SST2Dataset(train_df.sentence, train_df.label, tokenizer, max_length)\n",
    "dev_dataset = SST2Dataset(dev_df.sentence, dev_df.label, tokenizer, max_length)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=16,\n",
    "    pin_memory=True\n",
    ")\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=16,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(classifier_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "classifier_model, optimizer, train_loader, dev_loader = accelerator.prepare(\n",
    "    classifier_model, optimizer, train_loader, dev_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlmjpBinaryClassifier(\n",
       "  (llmjp_model): LlamaModel(\n",
       "    (embed_tokens): Embedding(99584, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((512,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((512,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((512,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (classifier): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "classifier_model.to(accelerator.device)\n",
    "classifier_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16838/16838 [04:41<00:00, 59.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.5494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16838/16838 [04:41<00:00, 59.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Loss: 0.5473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16838/16838 [04:41<00:00, 59.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Loss: 0.5470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16838/16838 [04:41<00:00, 59.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Average Loss: 0.5441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16838/16838 [04:41<00:00, 59.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Loss: 0.5461\n",
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        with accelerator.autocast():\n",
    "            logits = classifier_model(input_ids, attention_mask)\n",
    "            loss = criterion(logits.squeeze(-1), labels)\n",
    "        \n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 218/218 [00:04<00:00, 51.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development Set Accuracy: 0.7236\n",
      "\n",
      "Prediction examples:\n",
      "Sentence: it 's a charming and often affecting journey . \n",
      "True label: 1 (Positive)\n",
      "Predicted: 1 (Positive)\n",
      "Correct: True\n",
      "--------------------------------------------------\n",
      "Sentence: unflinchingly bleak and desperate \n",
      "True label: 0 (Negative)\n",
      "Predicted: 1 (Positive)\n",
      "Correct: False\n",
      "--------------------------------------------------\n",
      "Sentence: allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . \n",
      "True label: 1 (Positive)\n",
      "Predicted: 1 (Positive)\n",
      "Correct: True\n",
      "--------------------------------------------------\n",
      "Sentence: the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales . \n",
      "True label: 1 (Positive)\n",
      "Predicted: 1 (Positive)\n",
      "Correct: True\n",
      "--------------------------------------------------\n",
      "Sentence: it 's slow -- very , very slow . \n",
      "True label: 0 (Negative)\n",
      "Predicted: 0 (Negative)\n",
      "Correct: True\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 評価\n",
    "classifier_model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dev_loader, desc='Evaluating'):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        logits = classifier_model(input_ids, attention_mask)\n",
    "        predictions = torch.sigmoid(logits.squeeze(-1)) > 0.5\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 精度の計算\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f'Development Set Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# いくつかの予測例を表示\n",
    "print(\"\\nPrediction examples:\")\n",
    "for i in range(5):\n",
    "    sentence = dev_df['sentence'].iloc[i]\n",
    "    true_label = dev_df['label'].iloc[i]\n",
    "    pred_label = int(all_predictions[i])\n",
    "    \n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"True label: {true_label} ({'Positive' if true_label == 1 else 'Negative'})\")\n",
    "    print(f\"Predicted: {pred_label} ({'Positive' if pred_label == 1 else 'Negative'})\")\n",
    "    print(f\"Correct: {true_label == pred_label}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "UnREZD3nTWUr",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 98. ファインチューニング\n",
    "\n",
    "問題96のプロンプトに対して、正解の感情ラベルをテキストの応答として返すように事前学習済みモデルをファインチューニングせよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama3.2-3B-InstuructをQLoRAでチューニングした場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    LlamaForCausalLM, AutoTokenizer,\n",
    "    Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model, LoraConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "output_dir = 'output/qlora-llama3b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239aeab4a1bd401cb86d54d2c7ab0c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        inference_mode=False\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_prompt(sentence, label, tokenizer):\n",
    "    instruction = 'Determine if the sentiment of this sentence is positive or negative. Answer with only \"positive\" or \"negative\".'\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": sentence}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    response = \"positive\" if label == 1 else \"negative\"\n",
    "    full_text = prompt + response + tokenizer.eos_token\n",
    "\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, max_length=512):\n",
    "    texts = []\n",
    "    for sentence, label in zip(examples['sentence'], examples['label']):\n",
    "        full_text = create_training_prompt(sentence, label, tokenizer)\n",
    "        texts.append(full_text)\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 67349\n",
      "Development samples: 872\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('./data/SST-2/train.tsv', sep='\\t')\n",
    "dev_df = pd.read_csv('./data/SST-2/dev.tsv', sep='\\t')\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Development samples: {len(dev_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created successfully!\n",
      "Train dataset: Dataset({\n",
      "    features: ['sentence', 'label'],\n",
      "    num_rows: 67349\n",
      "})\n",
      "Dev dataset: Dataset({\n",
      "    features: ['sentence', 'label'],\n",
      "    num_rows: 872\n",
      "})\n",
      "\n",
      "First training example:\n",
      "Sentence: hide new secretions from the parental units \n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "\n",
    "print(\"Dataset created successfully!\")\n",
    "print(f\"Train dataset: {train_dataset}\")\n",
    "print(f\"Dev dataset: {dev_dataset}\")\n",
    "\n",
    "# データセットの内容確認\n",
    "print(f\"\\nFirst training example:\")\n",
    "print(f\"Sentence: {train_dataset[0]['sentence']}\")\n",
    "print(f\"Label: {train_dataset[0]['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4932ba05084a39b4cfe41d448dd0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36a8e10ecfb45889239f624a08efe72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization completed!\n",
      "Tokenized train dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 67349\n",
      "})\n",
      "Tokenized dev dataset: Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 872\n",
      "})\n",
      "\n",
      "First tokenized example:\n",
      "Input IDs length: 512\n",
      "Decoded text: <|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 Jun 2025\n",
      "\n",
      "Determine if the sentiment of this sentence is positive or negative. Answer with only \"positive\" or \"negative\".<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "hide new secretions from the parental units<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "negative<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>...\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = train_dataset.map(\n",
    "    lambda x: preprocess_function(x, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_dev = dev_dataset.map(\n",
    "    lambda x: preprocess_function(x, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=dev_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"Tokenization completed!\")\n",
    "print(f\"Tokenized train dataset: {tokenized_train}\")\n",
    "print(f\"Tokenized dev dataset: {tokenized_dev}\")\n",
    "\n",
    "# トークナイズされたデータの確認\n",
    "print(f\"\\nFirst tokenized example:\")\n",
    "example = tokenized_train[0]\n",
    "print(f\"Input IDs length: {len(example['input_ids'])}\")\n",
    "print(f\"Decoded text: {tokenizer.decode(example['input_ids'][:100])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collator created!\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # 因果言語モデルなのでFalse\n",
    "    pad_to_multiple_of=8,  # 効率化のため\n",
    ")\n",
    "\n",
    "print(\"Data collator created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured!\n",
      "Output directory: output/qlora-llama3b\n",
      "Batch size: 2\n",
      "Learning rate: 0.0002\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,  # メモリ制約に応じて調整\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10000,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=100,\n",
    "    fp16=True,  # メモリ節約\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,  # wandbなどの使用を無効化\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")\n",
    "print(f\"Output directory: {training_args.output_dir}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer created successfully!\n",
      "Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15596' max='33675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15596/33675 4:34:48 < 5:18:36, 0.95 it/s, Epoch 0.46/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.626300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.626300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.626300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.626300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.626300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.626300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.626300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.626300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.626300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.626300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.626300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.626300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tosshy/workspace/2025/ch10/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer created successfully!\")\n",
    "print(\"Starting fine-tuning...\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"Fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llm-jp-3-150m-instruct3をフルファインチューニングした場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer,\n",
    "    Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'llm-jp/llm-jp-3-150m-instruct3'\n",
    "output_dir = 'output/llm-jp150m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map={\"\": \"cuda:1\"},\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_prompt(sentence, label, tokenizer):\n",
    "    instruction = '以下の文がポジティブかネガティブのどちらなのか判定してください．\"ポジティブ\"か\"ネガティブ\"で回答してください．'\n",
    "    content = f'''{instruction}\n",
    "\n",
    "{sentence}\n",
    "    '''\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    response = \"ポジティブ\" if label == 1 else \"ネガティブ\"\n",
    "    full_text = prompt + response + tokenizer.eos_token\n",
    "\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "\n",
      "### 指示:\n",
      "以下の文がポジティブかネガティブのどちらなのか判定してください．\"ポジティブ\"か\"ネガティブ\"で回答してください．\n",
      "\n",
      "あいうえお\n",
      "    \n",
      "\n",
      "### 応答:\n",
      "ポジティブ</s>\n"
     ]
    }
   ],
   "source": [
    "print(create_training_prompt(\"あいうえお\", 1, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, max_length=512):\n",
    "    texts = []\n",
    "    for sentence, label in zip(examples['sentence'], examples['label']):\n",
    "        full_text = create_training_prompt(sentence, label, tokenizer)\n",
    "        texts.append(full_text)\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 67349\n",
      "Development samples: 872\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('./data/SST-2/train.tsv', sep='\\t')\n",
    "dev_df = pd.read_csv('./data/SST-2/dev.tsv', sep='\\t')\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Development samples: {len(dev_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created successfully!\n",
      "Train dataset: Dataset({\n",
      "    features: ['sentence', 'label'],\n",
      "    num_rows: 67349\n",
      "})\n",
      "Dev dataset: Dataset({\n",
      "    features: ['sentence', 'label'],\n",
      "    num_rows: 872\n",
      "})\n",
      "\n",
      "First training example:\n",
      "Sentence: hide new secretions from the parental units \n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "\n",
    "print(\"Dataset created successfully!\")\n",
    "print(f\"Train dataset: {train_dataset}\")\n",
    "print(f\"Dev dataset: {dev_dataset}\")\n",
    "\n",
    "# データセットの内容確認\n",
    "print(f\"\\nFirst training example:\")\n",
    "print(f\"Sentence: {train_dataset[0]['sentence']}\")\n",
    "print(f\"Label: {train_dataset[0]['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7b3a237e464b8bbd5decda2697e4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fffe5d822394b1298bc9939f29d0b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization completed!\n",
      "Tokenized train dataset: Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 67349\n",
      "})\n",
      "Tokenized dev dataset: Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 872\n",
      "})\n",
      "\n",
      "First tokenized example:\n",
      "Input IDs length: 512\n",
      "Decoded text: <s><s> \n",
      "\n",
      "### 指示:\n",
      "以下の文がポジティブかネガティブのどちらなのか判定してください．\"ポジティブ\"か\"ネガティブ\"で回答してください．\n",
      "\n",
      "hide new secretions from the parental units \n",
      "    \n",
      "\n",
      "### 応答:\n",
      "ネガティブ</s><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp><PAD|LLM-jp>...\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = train_dataset.map(\n",
    "    lambda x: preprocess_function(x, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_dev = dev_dataset.map(\n",
    "    lambda x: preprocess_function(x, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=dev_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"Tokenization completed!\")\n",
    "print(f\"Tokenized train dataset: {tokenized_train}\")\n",
    "print(f\"Tokenized dev dataset: {tokenized_dev}\")\n",
    "\n",
    "# トークナイズされたデータの確認\n",
    "print(f\"\\nFirst tokenized example:\")\n",
    "example = tokenized_train[0]\n",
    "print(f\"Input IDs length: {len(example['input_ids'])}\")\n",
    "print(f\"Decoded text: {tokenizer.decode(example['input_ids'][:100])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collator created!\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # 因果言語モデルなのでFalse\n",
    "    pad_to_multiple_of=8,  # 効率化のため\n",
    ")\n",
    "\n",
    "print(\"Data collator created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training arguments configured!\n",
      "Output directory: output/llm-jp150m\n",
      "Batch size: 16\n",
      "Learning rate: 0.0002\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=16,  # メモリ制約に応じて調整\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=100,\n",
    "    fp16=True,  # メモリ節約\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,  # wandbなどの使用を無効化\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")\n",
    "print(f\"Output directory: {training_args.output_dir}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer created successfully!\n",
      "Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='86' max='21050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   86/21050 00:23 < 1:39:12, 3.52 it/s, Epoch 0.02/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrainer created successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting fine-tuning...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFine-tuning completed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/2025/ch10/.venv/lib/python3.11/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/2025/ch10/.venv/lib/python3.11/site-packages/transformers/trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2555\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2561\u001b[39m ):\n\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2563\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2564\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer created successfully!\")\n",
    "print(\"Starting fine-tuning...\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"Fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "4f0St5Ce0l34",
    "tags": []
   },
   "source": [
    "## 99. 選好チューニング\n",
    "\n",
    "問題96のプロンプトに対して、正解の感情ラベルを含むテキストを望ましい応答、間違った感情ラベルを含むテキストを望ましくない応答として、事前学習済み言語モデルを選好チューニング (preference tuning) を実施せよ。選好チューニングのアルゴリズムとしては、近傍方策最適化 (PPO: Proximal Policy Optimization) や直接選好最適化 (DPO: Direct Preference Optimization) などが考えられる。\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a56790cc58c4155bb4fb62352fe6853": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c4e978f772946f293451de4484720ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da96a5f6901649958e682be81f939149",
      "placeholder": "​",
      "style": "IPY_MODEL_3f9c4fcaa07b42a0971b99205dd05458",
      "value": " 1/1 [00:00&lt;00:00, 25.67it/s]"
     }
    },
    "0f1172658036407993eb46da17554501": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd75431e52b54d9bb2fd7c06737c28f5",
      "placeholder": "​",
      "style": "IPY_MODEL_12c4ec93b2fc4b239b775d5b947b253f",
      "value": " 67349/0 [00:00&lt;00:00, 674165.72 examples/s]"
     }
    },
    "12c4ec93b2fc4b239b775d5b947b253f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1411c82bf2f3475a826a145fb9f89626": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "145b34f60df1429e8685bcc9b2f05be2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c8a470363f246d692c48ab4a502c4c7",
      "max": 872,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ae5b03ae867f4ffba662a8888b319926",
      "value": 872
     }
    },
    "14c91ad60bc84fb0a9c6cc1eb9365425": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b91de93f8ec46c2ac153f40f50ae2e0",
      "placeholder": "​",
      "style": "IPY_MODEL_f4367d9584aa42e0b4249b33af0435d9",
      "value": "Generating train split: "
     }
    },
    "16763c09bfa34f03821316fe7c9902e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c57bd417124c4faa8652b13e826405db",
       "IPY_MODEL_3248b0100e614e2a89b0e2dc13be0ad5",
       "IPY_MODEL_936c040e05bc4b2aadd82245d0c2f3b3"
      ],
      "layout": "IPY_MODEL_a1aac38df0bd43bf9a48a55be029f499"
     }
    },
    "1860eb6e26ca45aeab8ab4ab41334a25": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29245bfd86f0460ea2bfccaca75690a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e33f53fccce4875bbb245afe5b87cf3",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5cbbe2769e024a92a5aa718d8f889d0f",
      "value": 1
     }
    },
    "2b91de93f8ec46c2ac153f40f50ae2e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3248b0100e614e2a89b0e2dc13be0ad5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93d07d6d16da45b2a248ad37e51c3180",
      "max": 67349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a03ab49faf2a4027906a0e9f806bd2db",
      "value": 67349
     }
    },
    "3c41fe7af38f49f581ae359b5d970d23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f95aef5e2c74acd8ffbfad2543b2102",
      "placeholder": "​",
      "style": "IPY_MODEL_9a20391390dd4f96aaea37321b4dfb53",
      "value": " 872/0 [00:00&lt;00:00, 66375.69 examples/s]"
     }
    },
    "3f6a346c2abd42ea9fb5af9441376472": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97484a182f634ec8a711d516fdeb0c63",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d7453ed4be064ab586858e8dec61ffae",
      "value": 1
     }
    },
    "3f95aef5e2c74acd8ffbfad2543b2102": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f9c4fcaa07b42a0971b99205dd05458": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b8bf221e7974b65b4dde96219ce0f47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4bc6646a088f4bd7b58b33ae1bbab1ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d1eef416f5944c2911154fb770d869f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54587b2141984fd495bf55ea6df0d1a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d603aabb8b6443d2a66b94701858f523",
      "placeholder": "​",
      "style": "IPY_MODEL_1411c82bf2f3475a826a145fb9f89626",
      "value": "Generating dev split: "
     }
    },
    "5cbbe2769e024a92a5aa718d8f889d0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "64ebcabb9c26423f8cda28966d9bbedb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "678df530cc994774899eb213581f737b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c8a470363f246d692c48ab4a502c4c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75a29e01568b4272832f3e00dd92a707": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80837417e00b4fd3814524786898697c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "81b067d9308141c786bc70f709681ce7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_54587b2141984fd495bf55ea6df0d1a5",
       "IPY_MODEL_29245bfd86f0460ea2bfccaca75690a1",
       "IPY_MODEL_3c41fe7af38f49f581ae359b5d970d23"
      ],
      "layout": "IPY_MODEL_4bc6646a088f4bd7b58b33ae1bbab1ae"
     }
    },
    "8dec4107d6a44cdaae9020345e2c25a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8e33f53fccce4875bbb245afe5b87cf3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "936c040e05bc4b2aadd82245d0c2f3b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64ebcabb9c26423f8cda28966d9bbedb",
      "placeholder": "​",
      "style": "IPY_MODEL_678df530cc994774899eb213581f737b",
      "value": " 67349/67349 [00:03&lt;00:00, 20324.02 examples/s]"
     }
    },
    "93d07d6d16da45b2a248ad37e51c3180": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97484a182f634ec8a711d516fdeb0c63": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "9a20391390dd4f96aaea37321b4dfb53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a03ab49faf2a4027906a0e9f806bd2db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a1aac38df0bd43bf9a48a55be029f499": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3164d72e2d5431dbd7921b74290193a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a3cd48beab0741b99a7885f89ebe1181": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8eb7b58bb3a4bfc9d4dad2bc0857dbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f4da859c892b42cfa6c909e7dfe2df7d",
       "IPY_MODEL_df8979acad044b40bb55c3e00c418e05",
       "IPY_MODEL_0c4e978f772946f293451de4484720ed"
      ],
      "layout": "IPY_MODEL_b913e33aebe642cca6fa8a6ade682988"
     }
    },
    "ae5b03ae867f4ffba662a8888b319926": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b1763f6b7fc9430ab2f773279fdd954d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75a29e01568b4272832f3e00dd92a707",
      "placeholder": "​",
      "style": "IPY_MODEL_8dec4107d6a44cdaae9020345e2c25a4",
      "value": "Map: 100%"
     }
    },
    "b913e33aebe642cca6fa8a6ade682988": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd75431e52b54d9bb2fd7c06737c28f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0929c9ec77a48df8a7fd3e2f4539a51": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c57bd417124c4faa8652b13e826405db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1860eb6e26ca45aeab8ab4ab41334a25",
      "placeholder": "​",
      "style": "IPY_MODEL_4b8bf221e7974b65b4dde96219ce0f47",
      "value": "Map: 100%"
     }
    },
    "d5e5c6c318cc41c4ac40647e6d5a5106": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_14c91ad60bc84fb0a9c6cc1eb9365425",
       "IPY_MODEL_3f6a346c2abd42ea9fb5af9441376472",
       "IPY_MODEL_0f1172658036407993eb46da17554501"
      ],
      "layout": "IPY_MODEL_0a56790cc58c4155bb4fb62352fe6853"
     }
    },
    "d603aabb8b6443d2a66b94701858f523": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7453ed4be064ab586858e8dec61ffae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "da96a5f6901649958e682be81f939149": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "deee5780a392426096c2ff4f76aa1547": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "df8979acad044b40bb55c3e00c418e05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec97a156272345ba8d456c5f090d9412",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_deee5780a392426096c2ff4f76aa1547",
      "value": 1
     }
    },
    "e92b3187acda44e9b30051a9e410f43f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b1763f6b7fc9430ab2f773279fdd954d",
       "IPY_MODEL_145b34f60df1429e8685bcc9b2f05be2",
       "IPY_MODEL_fc589f87f65d4a3196cdd790bbdf7fa0"
      ],
      "layout": "IPY_MODEL_c0929c9ec77a48df8a7fd3e2f4539a51"
     }
    },
    "ec97a156272345ba8d456c5f090d9412": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4367d9584aa42e0b4249b33af0435d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4da859c892b42cfa6c909e7dfe2df7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d1eef416f5944c2911154fb770d869f",
      "placeholder": "​",
      "style": "IPY_MODEL_a3164d72e2d5431dbd7921b74290193a",
      "value": "100%"
     }
    },
    "fc589f87f65d4a3196cdd790bbdf7fa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3cd48beab0741b99a7885f89ebe1181",
      "placeholder": "​",
      "style": "IPY_MODEL_80837417e00b4fd3814524786898697c",
      "value": " 872/872 [00:00&lt;00:00, 9939.81 examples/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
