{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "ADPET68xjlzr",
    "tags": []
   },
   "source": [
    "# 第10章: 事前学習済み言語モデル（GPT型）\n",
    "\n",
    "本章では、GPT型（Transformerのデコーダ型）の事前学習済みモデルを利用して、言語生成、評判分析器（ポジネガ分類器）の構築、ファインチューニング、強化学習などに取り組む。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "dotenv_path = './.env'\n",
    "load_dotenv(dotenv_path)\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `experiments` has been saved to /home/tosshy/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /home/tosshy/.cache/huggingface/token\n",
      "Login successful.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token $HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "C1xKmMckti92",
    "tags": []
   },
   "source": [
    "## 90. 次単語予測\n",
    "\n",
    "“The movie was full of\"に続くトークン（トークン列ではなく一つのトークンであることに注意せよ）として適切なもの上位10個と、その確率（尤度）を求めよ。ただし、言語モデルへのプロンプトがどのようなトークン列に変換されたか、確認せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-3B-Instruct')\n",
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The movie was full of'\n",
    "\n",
    "encoded = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "tokenized = tokenizer.tokenize(sentence)\n",
    "print(tokenized)\n",
    "\n",
    "input_ids = tokenizer(sentence).input_ids\n",
    "print(input_ids)\n",
    "\n",
    "decoded = tokenizer.decode(input_ids)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実験：各系列における尤度が最大のものを選んだ結果\n",
    "model.to(device)\n",
    "encoded.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "encoded.to(device)\n",
    "\n",
    "print(encoded)\n",
    "\n",
    "outputs = model(**encoded)\n",
    "logits = outputs.logits # {batch_size, seq_len, vocab_size}\n",
    "# logits[:, k, :]はinput_ids[k-1]までを使って計算されたスコア\n",
    "next_token_logits = logits[:, -1, :]\n",
    "next_token_id = next_token_logits.argmax(-1).item()\n",
    "print(f'Next token id: {next_token_id}')\n",
    "next_token = tokenizer.decode([next_token_id])\n",
    "print(f'Next token: {next_token}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "top_k_probs, top_k_indices = torch.topk(next_token_probs, top_k) # (batch_size, top_k)\n",
    "\n",
    "print(f'Top {top_k} next tokens')\n",
    "for i in range(top_k):\n",
    "    token_id = top_k_indices[0, i].item()\n",
    "    token = tokenizer.decode([token_id])\n",
    "    prob = top_k_probs[0, i].item()\n",
    "    print(f'{i+1}. Token: {token}, Probability: {prob:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "s1RhOldA0meh",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 91. 続きのテキストの予測\n",
    "\n",
    "“The movie was full of\"に続くテキストを複数予測せよ。このとき、デコーディングの方法や温度パラメータ（temperature）を変えながら、予測される複数のテキストの変化を観察せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'The movie was full of'\n",
    "encoded = tokenizer(prompt, return_tensors='pt')\n",
    "input_ids = encoded.input_ids.to(device)\n",
    "attention_mask = encoded.attention_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy Searchに近い\n",
    "print('--- Default (Greedy-like) ---')\n",
    "outputs_default = model.generate(input_ids, attention_mask=attention_mask, max_length=50, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs_default[0], skip_special_tokens=True))\n",
    "print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプリング (do_sample=True) と Temperature\n",
    "# Temperature < 1.0 : より決定的\n",
    "# Temperature > 1.0 : よりランダム\n",
    "print('--- Sampling with Temperature (0.7) ---')\n",
    "outputs_temp_low = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(outputs_temp_low[0], skip_special_tokens=True))\n",
    "print('-' * 30)\n",
    "\n",
    "print('--- Sampling with Temperature (1.5) ---')\n",
    "outputs_temp_high = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    temperature=1.5,\n",
    "    top_k=50,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(outputs_temp_high[0], skip_special_tokens=True))\n",
    "print('-' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-k サンプリング\n",
    "# 次のトークンを予測する際に，確率上位k個の中からサンプリングする\n",
    "print('--- Top-k Sampling (k=30) ---')\n",
    "outputs_top_k = model.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    top_k=30,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(outputs_top_k[0], skip_special_tokens=True))\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-p (Nucleus) サンプリング\n",
    "# 確率の累積がpを超える最小のトークンセットからサンプリングする\n",
    "print('--- Top-p (Nucleus) Sampling (p=0.9) ---')\n",
    "outputs_top_p = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    top_k=0, # top_kとtop_pは通常どちらか一方を指定するか、top_k=0でtop_pを有効にする\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(outputs_top_p[0], skip_special_tokens=True))\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ビームサーチ\n",
    "# 複数の候補 (ビーム) を保持しながら探索する\n",
    "print(\"--- Beam Search (num_beams=5) ---\")\n",
    "outputs_beam = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    early_stopping=True, # EOSトークンが出たら早めに打ち切る\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(outputs_beam[0], skip_special_tokens=True))\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数の異なる出力を得るために num_return_sequences を使用\n",
    "print(\"--- Beam Search with num_return_sequences=3 ---\")\n",
    "outputs_beam_multiple = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    num_beams=5,\n",
    "    num_return_sequences=3,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "for i, output in enumerate(outputs_beam_multiple):\n",
    "    print(f\"Output {i+1}: {tokenizer.decode(output, skip_special_tokens=True)}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "7ZFadg6B8VdA",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 92. 予測されたテキストの確率を計算\n",
    "\n",
    "“The movie was full of\"に続くテキストを予測し、生成された各単語の尤度を表示せよ（生成されるテキストが長いと出力が読みにくくなるので、適当な長さで生成を打ち切るとよい）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'The movie was full of'\n",
    "encoded = tokenizer(prompt, return_tensors='pt')\n",
    "input_ids = encoded.input_ids.to(device)\n",
    "attention_mask = encoded.attention_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_ids, \n",
    "    attention_mask=attention_mask, \n",
    "    max_length=20, \n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True\n",
    ")\n",
    "\n",
    "sequences = list(outputs.sequences.squeeze(0))\n",
    "scores = outputs.scores\n",
    "\n",
    "prompt_len = input_ids.shape[1]\n",
    "\n",
    "print(f\"Input prompt: {tokenizer.decode(sequences[:prompt_len], skip_special_tokens=True)}\")\n",
    "print(\"Generated tokens and their probabilities:\")\n",
    "\n",
    "for k in range(len(scores)):\n",
    "    current_token_idx_in_sequence = prompt_len + k\n",
    "\n",
    "    generated_token_id = sequences[current_token_idx_in_sequence]\n",
    "\n",
    "    generated_token_str = tokenizer.decode([generated_token_id])\n",
    "\n",
    "    step_logits = scores[k]\n",
    "\n",
    "    step_probs = torch.softmax(step_logits, dim=-1).squeeze()\n",
    "\n",
    "    prob_of_generated_token = step_probs[generated_token_id].item()\n",
    "\n",
    "    print(f\"Token: '{generated_token_str}', Probability: {prob_of_generated_token:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "FvNCTMj6OegF",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 93. パープレキシティ\n",
    "\n",
    "適当な文を準備して、事前学習済み言語モデルでパープレキシティを測定せよ。例えば、\n",
    "\n",
    "+ The movie was full of surprises\n",
    "+ The movies were full of surprises\n",
    "+ The movie were full of surprises\n",
    "+ The movies was full of surprises\n",
    "\n",
    "の4文に対して、パープレキシティを測定して観察せよ（最後の2つの文は故意に文法的な間違いを入れた）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'The movie was full of surprises',\n",
    "    'The movies were full of surprises',\n",
    "    'The movie were full of surprises',\n",
    "    'The movies was full of surprises'\n",
    "]\n",
    "\n",
    "encoded = tokenizer(sentences, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "encoded.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encoded.input_ids\n",
    "outputs = model(**encoded)\n",
    "logits = outputs.logits\n",
    "\n",
    "for i in range(logits.shape[0]):\n",
    "    current_prediction_logits = logits[i, :-1, :]\n",
    "    current_target_ids = input_ids[i, 1:]\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "    mean_neg_log_likelihood = criterion(current_prediction_logits, current_target_ids)\n",
    "\n",
    "    ppl = torch.exp(mean_neg_log_likelihood)\n",
    "    print(f'Sentence: {sentences[i]}, Perplexity: {ppl.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-7fB-n9suYg"
   },
   "source": [
    "## 94. チャットテンプレート\n",
    "\n",
    "\"What do you call a sweet eaten after dinner?\"という問いかけに対する応答を生成するため、チャットテンプレートを適用し、言語モデルに与えるべきプロンプトを作成せよ。また、そのプロンプトに対する応答を生成し、表示せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = 'Answer the following question.'\n",
    "text = 'What do you call a sweet eaten after dinner?'\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": instruction},\n",
    "    {\"role\": \"user\", \"content\": text}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print('Generated prompt:')\n",
    "print(prompt)\n",
    "print('=' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print('Model response:')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "PT-bk0XWIZ2E",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 95. マルチターンのチャット\n",
    "\n",
    "問題94で生成された応答に対して、追加で\"Please give me the plural form of the word with its spelling in reverse order.\"と問いかけたときの応答を生成・表示せよ。また、その時に言語モデルに与えるプロンプトを確認せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_response = response\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": instruction},\n",
    "    {\"role\": \"user\", \"content\": \"What do you call a sweet eaten after dinner?\"},\n",
    "    {\"role\": \"assistant\", \"content\": previous_response},\n",
    "    {\"role\": \"user\", \"content\": \"Please give me the plural form of the word with its spelling in reverse order.\"}\n",
    "]\n",
    "\n",
    "multi_turn_prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print('Multi-turn chat prompt')\n",
    "print(multi_turn_prompt)\n",
    "print('=' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(multi_turn_prompt, return_tensors='pt').to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "multi_turn_response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print('Multi-turn response:')\n",
    "print(multi_turn_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "qH0YortL0afd",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 96. プロンプトによる感情分析\n",
    "\n",
    "事前学習済み言語モデルで感情分析を行いたい。テキストを含むプロンプトを事前学習済み言語モデルに与え、（ファインチューニングは行わずに）テキストのポジネガを予測するという戦略で、[SST-2](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip)の開発データにおける正解率を測定せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_path = './data/SST-2/train.tsv'\n",
    "dev_path = './data/SST-2/dev.tsv'\n",
    "\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "dev_df = pd.read_csv(dev_path, sep='\\t')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "dev_sentences = dev_df.sentence\n",
    "dev_labels = dev_df.label\n",
    "\n",
    "predictions = []\n",
    "\n",
    "instruction = 'Determine if the sentiment of this sentence is positive or negative. Answer with only \"positive\" or \"negative\".'\n",
    "\n",
    "for i in tqdm(range(len(dev_sentences)), desc='Processing sentences'):\n",
    "    sentence = dev_sentences.iloc[i]\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": sentence}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=15,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    predictions.append(response.strip())\n",
    "\n",
    "correct = 0\n",
    "unable_to_predict = 0\n",
    "for i, pred in enumerate(predictions):\n",
    "    true_label = dev_labels.iloc[i]\n",
    "    pred_lower = pred.lower()\n",
    "\n",
    "    if true_label == 1 and 'positive' in pred_lower:\n",
    "        correct += 1\n",
    "    elif true_label == 0 and 'negative' in pred_lower:\n",
    "        correct += 1\n",
    "    elif 'positive' not in pred_lower and 'negative' not in pred_lower:\n",
    "        unable_to_predict += 1\n",
    "\n",
    "total = len(predictions)\n",
    "accuracy = correct / total\n",
    "unable_rate = unable_to_predict / total\n",
    "\n",
    "print(f'Total samples: {total}')\n",
    "print(f'Correct predictions: {correct}')\n",
    "print(f'Unable to predict: {unable_to_predict}')\n",
    "print(f'Wrong predictions: {total - correct - unable_to_predict}')\n",
    "print(f'Accuracy: {accuracy:.4f} ({correct}/{total})')\n",
    "print(f'Unable to predict rate: {unable_rate:.4f} ({unable_to_predict}/{total})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unable_to_predict > 0:\n",
    "    print(f'\\nExamples of unable to predict:')\n",
    "    count = 0\n",
    "    for i, pred in enumerate(predictions):\n",
    "        pred_lower = pred.lower()\n",
    "        if 'positive' not in pred_lower and 'negative' not in pred_lower:\n",
    "            print(f'  Sentence: \"{dev_sentences.iloc[i]}\"')\n",
    "            print(f'  Prediction: \"{pred}\"')\n",
    "            print(f'  True label: {dev_labels.iloc[i]}')\n",
    "            print('-' * 50)\n",
    "            count += 1\n",
    "            if count >= 5:  # 最初の5件だけ表示\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "giA6FivrKaSf",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 97. 埋め込みに基づく感情分析\n",
    "\n",
    "事前学習済み言語モデルでテキストをベクトルで表現（エンコード）し、そのベクトルにフィードフォワード層を通すことで極性ラベルを予測するモデルを学習せよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, LlamaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67344</th>\n",
       "      <td>a delightful comedy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67345</th>\n",
       "      <td>anguish , anger and frustration</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67346</th>\n",
       "      <td>at achieving the modest , crowd-pleasing goals...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67347</th>\n",
       "      <td>a patient viewer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67348</th>\n",
       "      <td>this new jangle of noise , mayhem and stupidit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67349 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label\n",
       "0           hide new secretions from the parental units       0\n",
       "1                   contains no wit , only labored gags       0\n",
       "2      that loves its characters and communicates som...      1\n",
       "3      remains utterly satisfied to remain the same t...      0\n",
       "4      on the worst revenge-of-the-nerds clichés the ...      0\n",
       "...                                                  ...    ...\n",
       "67344                               a delightful comedy       1\n",
       "67345                   anguish , anger and frustration       0\n",
       "67346  at achieving the modest , crowd-pleasing goals...      1\n",
       "67347                                  a patient viewer       1\n",
       "67348  this new jangle of noise , mayhem and stupidit...      0\n",
       "\n",
       "[67349 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_path = './data/SST-2/train.tsv'\n",
    "dev_path = './data/SST-2/dev.tsv'\n",
    "\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "dev_df = pd.read_csv(dev_path, sep='\\t')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaBinaryClassifier(nn.Module):\n",
    "    def __init__(self, llama_model, hidden_size):\n",
    "        super().__init__()\n",
    "        self.llama_model = llama_model\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        for param in self.llama_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.llama_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            use_cache=False\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.hidden_states[-1]\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            masked_hidden = hidden_states.clone()\n",
    "            masked_hidden[attention_mask == 0] = float('-inf')\n",
    "            pooled = torch.max(masked_hidden, dim=1)[0]\n",
    "        else:\n",
    "            pooled = torch.max(hidden_states, dim=1)[0]\n",
    "\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SST2Dataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=256):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = str(self.sentences.iloc[idx])\n",
    "        label = self.labels.iloc[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding.input_ids.flatten(),\n",
    "            'attention_mask': encoding.attention_mask.flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        print(f\"Available VRAM: {torch.cuda.memory_allocated() / 1e9:.1f} GB\")\n",
    "        print(f\"Cached VRAM: {torch.cuda.memory_reserved() / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA RTX A4500\n",
      "Total VRAM: 21.0 GB\n",
      "Available VRAM: 0.0 GB\n",
      "Cached VRAM: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and base LlamaModel...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eed8f3be3d84939ae9b8573e57cda6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Loading tokenizer and base LlamaModel...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-3B-Instruct')\n",
    "model = LlamaModel.from_pretrained(\n",
    "    'meta-llama/Llama-3.2-3B-Instruct',\n",
    "    device_map={\"\": 1},\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:\n",
      "layers.0.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.0.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.0.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.0.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.0.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.0.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.0.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.1.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.2.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.3.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.4.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.5.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.6.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.7.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.8.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.9.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.10.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.11.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.12.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.12.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.12.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.12.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.12.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.12.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.12.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.13.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.13.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.13.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.13.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.13.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.13.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.13.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.14.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.14.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.14.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.14.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.14.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.14.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.14.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.15.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.15.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.15.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.15.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.15.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.15.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.15.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.16.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.16.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.16.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.16.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.16.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.16.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.16.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.17.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.17.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.17.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.17.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.17.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.17.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.17.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.18.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.18.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.18.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.18.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.18.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.18.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.18.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.19.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.19.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.19.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.19.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.19.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.19.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.19.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.20.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.20.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.20.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.20.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.20.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.20.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.20.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.21.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.21.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.21.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.21.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.21.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.21.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.21.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.22.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.22.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.22.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.22.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.22.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.22.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.22.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.23.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.23.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.23.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.23.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.23.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.23.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.23.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.24.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.24.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.24.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.24.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.24.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.24.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.24.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.25.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.25.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.25.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.25.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.25.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.25.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.25.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.26.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.26.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.26.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.26.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.26.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.26.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.26.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.27.self_attn.q_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.27.self_attn.k_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.27.self_attn.v_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.27.self_attn.o_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.27.mlp.gate_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.27.mlp.up_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "layers.27.mlp.down_proj: <class 'torch.nn.modules.linear.Linear'>\n",
      "\n",
      "Model attributes:\n",
      "['_convert_head_mask_to_5d', '_copy_lm_head_original_to_resized', '_get_resized_lm_head', '_init_added_lm_head_bias_with_mean', '_init_added_lm_head_weights_with_mean', 'get_head_mask', 'prune_heads']\n",
      "\n",
      "Model config: LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Model structure:\")\n",
    "for name, module in model.named_modules():\n",
    "    if 'head' in name.lower() or 'output' in name.lower() or 'proj' in name.lower():\n",
    "        print(f\"{name}: {type(module)}\")\n",
    "\n",
    "print(\"\\nModel attributes:\")\n",
    "print([attr for attr in dir(model) if 'head' in attr.lower()])\n",
    "\n",
    "print(f\"\\nModel config: {model.config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up padding token...\n",
      "Set pad_token to: <|eot_id|>\n",
      "Set model pad_token_id to: 128009\n",
      "tokenizer.pad_token: <|eot_id|>\n",
      "tokenizer.pad_token_id: 128009\n",
      "tokenizer.eos_token: <|eot_id|>\n",
      "tokenizer.eos_token_id: 128009\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up padding token...\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Set pad_token to: {tokenizer.pad_token}\")\n",
    "\n",
    "if hasattr(model, 'config'):\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    print(f\"Set model pad_token_id to: {model.config.pad_token_id}\")\n",
    "\n",
    "print(f\"tokenizer.pad_token: {tokenizer.pad_token}\")\n",
    "print(f\"tokenizer.pad_token_id: {tokenizer.pad_token_id}\")\n",
    "print(f\"tokenizer.eos_token: {tokenizer.eos_token}\")\n",
    "print(f\"tokenizer.eos_token_id: {tokenizer.eos_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden size: 3072\n"
     ]
    }
   ],
   "source": [
    "hidden_size = model.config.hidden_size\n",
    "print(f'Hidden size: {hidden_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_4bit(W: torch.Tensor):\n",
    "    # per-row scale\n",
    "    maxv = W.abs().amax(dim=1, keepdim=True) # (out, 1)\n",
    "    scale = maxv / 7.0\n",
    "    Wq = torch.clamp((W / scale).round(), -8, 7).to(torch.int8)\n",
    "    return Wq, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantLinear(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear):\n",
    "        super().__init__()\n",
    "        W = linear.weight.data # (out, in)\n",
    "        Wq, scale = quantize_4bit(W)\n",
    "        self.register_buffer('Wq', Wq)\n",
    "        self.register_buffer('scale', scale)\n",
    "        if linear.bias is not None:\n",
    "            self.bias = nn.Parameter(linear.bias.data.clone())\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # dequantize on the fly\n",
    "        W = self.Wq.float() * self.scale\n",
    "        return F.linear(x, W, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, r=8, alpha=16):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "        self.scaling = alpha / r\n",
    "        self.down = nn.Linear(in_features, r, bias=False)\n",
    "        self.up   = nn.Linear(r, out_features, bias=False)\n",
    "        nn.init.kaiming_uniform_(self.down.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.up.weight)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.up(self.down(x)) * self.scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantLoRALinear(nn.Module):\n",
    "    def __init__(self, linear: nn.Linear, r=8, alpha=16):\n",
    "        super().__init__()\n",
    "        self.quant = QuantLinear(linear)\n",
    "        self.lora = LoRALinear(linear.in_features, linear.out_features, r=r, alpha=alpha)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.quant(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_qlora(\n",
    "        model, \n",
    "        target_substrings=(\n",
    "            # Attention\n",
    "            'q_proj','v_proj','k_proj','o_proj'\n",
    "            # MLP (FFN)\n",
    "            # 'gate_proj', 'down_proj', 'up_proj'\n",
    "        )\n",
    "    ):\n",
    "    for name, module in list(model.named_modules()):\n",
    "        if isinstance(module, nn.Linear) and any(s in name for s in target_substrings):\n",
    "            parent_name, attr = name.rsplit('.', 1)\n",
    "            parent_mod = dict(model.named_modules())[parent_name]\n",
    "            setattr(parent_mod,\n",
    "                    attr,\n",
    "                    QuantLoRALinear(module, r=8, alpha=16)\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model = LlamaBinaryClassifier(model, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in classifier_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "apply_qlora(classifier_model)\n",
    "for p in classifier_model.modules():\n",
    "    if isinstance(p, LoRALinear):\n",
    "        for sub in (p.down, p.up):\n",
    "            for q in sub.parameters():\n",
    "                q.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "batch_size = 4\n",
    "\n",
    "train_dataset = SST2Dataset(train_df.sentence, train_df.label, tokenizer, max_length)\n",
    "dev_dataset = SST2Dataset(dev_df.sentence, dev_df.label, tokenizer, max_length)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=16,\n",
    "    pin_memory=True\n",
    ")\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=16,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "\n",
    "lora_params = [p for p in classifier_model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(lora_params, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA RTX A4500\n",
      "Total VRAM: 21.0 GB\n",
      "Available VRAM: 0.0 GB\n",
      "Cached VRAM: 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "classifier_model, optimizer, train_loader, dev_loader = accelerator.prepare(\n",
    "    classifier_model, optimizer, train_loader, dev_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaBinaryClassifier(\n",
       "  (llama_model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): QuantLoRALinear(\n",
       "            (quant): QuantLinear()\n",
       "            (lora): LoRALinear(\n",
       "              (down): Linear(in_features=3072, out_features=8, bias=False)\n",
       "              (up): Linear(in_features=8, out_features=3072, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (k_proj): QuantLoRALinear(\n",
       "            (quant): QuantLinear()\n",
       "            (lora): LoRALinear(\n",
       "              (down): Linear(in_features=3072, out_features=8, bias=False)\n",
       "              (up): Linear(in_features=8, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (v_proj): QuantLoRALinear(\n",
       "            (quant): QuantLinear()\n",
       "            (lora): LoRALinear(\n",
       "              (down): Linear(in_features=3072, out_features=8, bias=False)\n",
       "              (up): Linear(in_features=8, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (o_proj): QuantLoRALinear(\n",
       "            (quant): QuantLinear()\n",
       "            (lora): LoRALinear(\n",
       "              (down): Linear(in_features=3072, out_features=8, bias=False)\n",
       "              (up): Linear(in_features=8, out_features=3072, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (classifier): Linear(in_features=3072, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "classifier_model.to(accelerator.device)\n",
    "classifier_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|███████████████████████████████████████████████████████████████| 16838/16838 [5:19:56<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.1552\n",
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        with accelerator.autocast():\n",
    "            logits = classifier_model(input_ids, attention_mask)\n",
    "            loss = criterion(logits.squeeze(-1), labels)\n",
    "        \n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 218/218 [02:07<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development Set Accuracy: 0.9633\n",
      "GPU: NVIDIA RTX A4500\n",
      "Total VRAM: 21.0 GB\n",
      "Available VRAM: 10.8 GB\n",
      "Cached VRAM: 19.8 GB\n",
      "\n",
      "Prediction examples:\n",
      "Sentence: it 's a charming and often affecting journey . \n",
      "True label: 1 (Positive)\n",
      "Predicted: 1 (Positive)\n",
      "Correct: True\n",
      "--------------------------------------------------\n",
      "Sentence: unflinchingly bleak and desperate \n",
      "True label: 0 (Negative)\n",
      "Predicted: 0 (Negative)\n",
      "Correct: True\n",
      "--------------------------------------------------\n",
      "Sentence: allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker . \n",
      "True label: 1 (Positive)\n",
      "Predicted: 1 (Positive)\n",
      "Correct: True\n",
      "--------------------------------------------------\n",
      "Sentence: the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales . \n",
      "True label: 1 (Positive)\n",
      "Predicted: 1 (Positive)\n",
      "Correct: True\n",
      "--------------------------------------------------\n",
      "Sentence: it 's slow -- very , very slow . \n",
      "True label: 0 (Negative)\n",
      "Predicted: 0 (Negative)\n",
      "Correct: True\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 評価\n",
    "classifier_model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dev_loader, desc='Evaluating'):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        logits = classifier_model(input_ids, attention_mask)\n",
    "        predictions = torch.sigmoid(logits.squeeze(-1)) > 0.5\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 精度の計算\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f'Development Set Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# メモリ使用量の最終チェック\n",
    "check_gpu_memory()\n",
    "\n",
    "# いくつかの予測例を表示\n",
    "print(\"\\nPrediction examples:\")\n",
    "for i in range(5):\n",
    "    sentence = dev_df['sentence'].iloc[i]\n",
    "    true_label = dev_df['label'].iloc[i]\n",
    "    pred_label = int(all_predictions[i])\n",
    "    \n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"True label: {true_label} ({'Positive' if true_label == 1 else 'Negative'})\")\n",
    "    print(f\"Predicted: {pred_label} ({'Positive' if pred_label == 1 else 'Negative'})\")\n",
    "    print(f\"Correct: {true_label == pred_label}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "UnREZD3nTWUr",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 98. ファインチューニング\n",
    "\n",
    "問題96のプロンプトに対して、正解の感情ラベルをテキストの応答として返すように事前学習済みモデルをファインチューニングせよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    LlamaForCausalLM, LlamaTokenizer,\n",
    "    Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model, LoraConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './data/SST-2/train.tsv'\n",
    "dev_path = './data/SST-2/dev.tsv'\n",
    "\n",
    "train_df = pd.read_csv(train_path, sep='\\t')\n",
    "dev_df = pd.read_csv(dev_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "output_dir = 'output/qlora-llama3b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config={\"bnb_4bit_use_double_quant\": True}\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        inference_mode=False\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_prompt(sentence, label, tokenizer):\n",
    "    instruction = 'Determine if the sentiment of this sentence is positive or negative. Answer with only \"positive\" or \"negative\".'\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": sentence}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    response = \"positive\" if label == 1 else \"negative\"\n",
    "    full_text = prompt + response + tokenizer.eos_token\n",
    "\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, max_length=512):\n",
    "    texts = []\n",
    "    for sentence, label in zip(examples['sentence'], examples['label']):\n",
    "        full_text = create_training_prompt(sentence, label, tokenizer)\n",
    "        texts.append(full_text)\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        trucation=True,\n",
    "        max_length=max_length,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/SST-2/train.tsv', sep='\\t')\n",
    "dev_df = pd.read_csv('./data/SST-2/dev.tsv', sep='\\t')\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Development samples: {len(dev_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "\n",
    "print(\"Dataset created successfully!\")\n",
    "print(f\"Train dataset: {train_dataset}\")\n",
    "print(f\"Dev dataset: {dev_dataset}\")\n",
    "\n",
    "# データセットの内容確認\n",
    "print(f\"\\nFirst training example:\")\n",
    "print(f\"Sentence: {train_dataset[0]['sentence']}\")\n",
    "print(f\"Label: {train_dataset[0]['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = train_dataset.map(\n",
    "    lambda x: preprocess_function(x, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_dev = dev_dataset.map(\n",
    "    lambda x: preprocess_function(x, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=dev_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"Tokenization completed!\")\n",
    "print(f\"Tokenized train dataset: {tokenized_train}\")\n",
    "print(f\"Tokenized dev dataset: {tokenized_dev}\")\n",
    "\n",
    "# トークナイズされたデータの確認\n",
    "print(f\"\\nFirst tokenized example:\")\n",
    "example = tokenized_train[0]\n",
    "print(f\"Input IDs length: {len(example['input_ids'])}\")\n",
    "print(f\"Decoded text: {tokenizer.decode(example['input_ids'][:100])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # 因果言語モデルなのでFalse\n",
    "    pad_to_multiple_of=8,  # 効率化のため\n",
    ")\n",
    "\n",
    "print(\"Data collator created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,  # メモリ制約に応じて調整\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=100,\n",
    "    fp16=True,  # メモリ節約\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,  # wandbなどの使用を無効化\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured!\")\n",
    "print(f\"Output directory: {training_args.output_dir}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer created successfully!\")\n",
    "print(\"Starting fine-tuning...\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"Fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "4f0St5Ce0l34",
    "tags": []
   },
   "source": [
    "## 99. 選好チューニング\n",
    "\n",
    "問題96のプロンプトに対して、正解の感情ラベルを含むテキストを望ましい応答、間違った感情ラベルを含むテキストを望ましくない応答として、事前学習済み言語モデルを選好チューニング (preference tuning) を実施せよ。選好チューニングのアルゴリズムとしては、近傍方策最適化 (PPO: Proximal Policy Optimization) や直接選好最適化 (DPO: Direct Preference Optimization) などが考えられる。\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a56790cc58c4155bb4fb62352fe6853": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c4e978f772946f293451de4484720ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da96a5f6901649958e682be81f939149",
      "placeholder": "​",
      "style": "IPY_MODEL_3f9c4fcaa07b42a0971b99205dd05458",
      "value": " 1/1 [00:00&lt;00:00, 25.67it/s]"
     }
    },
    "0f1172658036407993eb46da17554501": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd75431e52b54d9bb2fd7c06737c28f5",
      "placeholder": "​",
      "style": "IPY_MODEL_12c4ec93b2fc4b239b775d5b947b253f",
      "value": " 67349/0 [00:00&lt;00:00, 674165.72 examples/s]"
     }
    },
    "12c4ec93b2fc4b239b775d5b947b253f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1411c82bf2f3475a826a145fb9f89626": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "145b34f60df1429e8685bcc9b2f05be2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c8a470363f246d692c48ab4a502c4c7",
      "max": 872,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ae5b03ae867f4ffba662a8888b319926",
      "value": 872
     }
    },
    "14c91ad60bc84fb0a9c6cc1eb9365425": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b91de93f8ec46c2ac153f40f50ae2e0",
      "placeholder": "​",
      "style": "IPY_MODEL_f4367d9584aa42e0b4249b33af0435d9",
      "value": "Generating train split: "
     }
    },
    "16763c09bfa34f03821316fe7c9902e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c57bd417124c4faa8652b13e826405db",
       "IPY_MODEL_3248b0100e614e2a89b0e2dc13be0ad5",
       "IPY_MODEL_936c040e05bc4b2aadd82245d0c2f3b3"
      ],
      "layout": "IPY_MODEL_a1aac38df0bd43bf9a48a55be029f499"
     }
    },
    "1860eb6e26ca45aeab8ab4ab41334a25": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29245bfd86f0460ea2bfccaca75690a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e33f53fccce4875bbb245afe5b87cf3",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5cbbe2769e024a92a5aa718d8f889d0f",
      "value": 1
     }
    },
    "2b91de93f8ec46c2ac153f40f50ae2e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3248b0100e614e2a89b0e2dc13be0ad5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93d07d6d16da45b2a248ad37e51c3180",
      "max": 67349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a03ab49faf2a4027906a0e9f806bd2db",
      "value": 67349
     }
    },
    "3c41fe7af38f49f581ae359b5d970d23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f95aef5e2c74acd8ffbfad2543b2102",
      "placeholder": "​",
      "style": "IPY_MODEL_9a20391390dd4f96aaea37321b4dfb53",
      "value": " 872/0 [00:00&lt;00:00, 66375.69 examples/s]"
     }
    },
    "3f6a346c2abd42ea9fb5af9441376472": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97484a182f634ec8a711d516fdeb0c63",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d7453ed4be064ab586858e8dec61ffae",
      "value": 1
     }
    },
    "3f95aef5e2c74acd8ffbfad2543b2102": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f9c4fcaa07b42a0971b99205dd05458": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b8bf221e7974b65b4dde96219ce0f47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4bc6646a088f4bd7b58b33ae1bbab1ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d1eef416f5944c2911154fb770d869f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54587b2141984fd495bf55ea6df0d1a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d603aabb8b6443d2a66b94701858f523",
      "placeholder": "​",
      "style": "IPY_MODEL_1411c82bf2f3475a826a145fb9f89626",
      "value": "Generating dev split: "
     }
    },
    "5cbbe2769e024a92a5aa718d8f889d0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "64ebcabb9c26423f8cda28966d9bbedb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "678df530cc994774899eb213581f737b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c8a470363f246d692c48ab4a502c4c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75a29e01568b4272832f3e00dd92a707": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80837417e00b4fd3814524786898697c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "81b067d9308141c786bc70f709681ce7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_54587b2141984fd495bf55ea6df0d1a5",
       "IPY_MODEL_29245bfd86f0460ea2bfccaca75690a1",
       "IPY_MODEL_3c41fe7af38f49f581ae359b5d970d23"
      ],
      "layout": "IPY_MODEL_4bc6646a088f4bd7b58b33ae1bbab1ae"
     }
    },
    "8dec4107d6a44cdaae9020345e2c25a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8e33f53fccce4875bbb245afe5b87cf3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "936c040e05bc4b2aadd82245d0c2f3b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64ebcabb9c26423f8cda28966d9bbedb",
      "placeholder": "​",
      "style": "IPY_MODEL_678df530cc994774899eb213581f737b",
      "value": " 67349/67349 [00:03&lt;00:00, 20324.02 examples/s]"
     }
    },
    "93d07d6d16da45b2a248ad37e51c3180": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97484a182f634ec8a711d516fdeb0c63": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "9a20391390dd4f96aaea37321b4dfb53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a03ab49faf2a4027906a0e9f806bd2db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a1aac38df0bd43bf9a48a55be029f499": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3164d72e2d5431dbd7921b74290193a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a3cd48beab0741b99a7885f89ebe1181": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8eb7b58bb3a4bfc9d4dad2bc0857dbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f4da859c892b42cfa6c909e7dfe2df7d",
       "IPY_MODEL_df8979acad044b40bb55c3e00c418e05",
       "IPY_MODEL_0c4e978f772946f293451de4484720ed"
      ],
      "layout": "IPY_MODEL_b913e33aebe642cca6fa8a6ade682988"
     }
    },
    "ae5b03ae867f4ffba662a8888b319926": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b1763f6b7fc9430ab2f773279fdd954d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75a29e01568b4272832f3e00dd92a707",
      "placeholder": "​",
      "style": "IPY_MODEL_8dec4107d6a44cdaae9020345e2c25a4",
      "value": "Map: 100%"
     }
    },
    "b913e33aebe642cca6fa8a6ade682988": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd75431e52b54d9bb2fd7c06737c28f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0929c9ec77a48df8a7fd3e2f4539a51": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c57bd417124c4faa8652b13e826405db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1860eb6e26ca45aeab8ab4ab41334a25",
      "placeholder": "​",
      "style": "IPY_MODEL_4b8bf221e7974b65b4dde96219ce0f47",
      "value": "Map: 100%"
     }
    },
    "d5e5c6c318cc41c4ac40647e6d5a5106": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_14c91ad60bc84fb0a9c6cc1eb9365425",
       "IPY_MODEL_3f6a346c2abd42ea9fb5af9441376472",
       "IPY_MODEL_0f1172658036407993eb46da17554501"
      ],
      "layout": "IPY_MODEL_0a56790cc58c4155bb4fb62352fe6853"
     }
    },
    "d603aabb8b6443d2a66b94701858f523": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7453ed4be064ab586858e8dec61ffae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "da96a5f6901649958e682be81f939149": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "deee5780a392426096c2ff4f76aa1547": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "df8979acad044b40bb55c3e00c418e05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec97a156272345ba8d456c5f090d9412",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_deee5780a392426096c2ff4f76aa1547",
      "value": 1
     }
    },
    "e92b3187acda44e9b30051a9e410f43f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b1763f6b7fc9430ab2f773279fdd954d",
       "IPY_MODEL_145b34f60df1429e8685bcc9b2f05be2",
       "IPY_MODEL_fc589f87f65d4a3196cdd790bbdf7fa0"
      ],
      "layout": "IPY_MODEL_c0929c9ec77a48df8a7fd3e2f4539a51"
     }
    },
    "ec97a156272345ba8d456c5f090d9412": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4367d9584aa42e0b4249b33af0435d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4da859c892b42cfa6c909e7dfe2df7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d1eef416f5944c2911154fb770d869f",
      "placeholder": "​",
      "style": "IPY_MODEL_a3164d72e2d5431dbd7921b74290193a",
      "value": "100%"
     }
    },
    "fc589f87f65d4a3196cdd790bbdf7fa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3cd48beab0741b99a7885f89ebe1181",
      "placeholder": "​",
      "style": "IPY_MODEL_80837417e00b4fd3814524786898697c",
      "value": " 872/872 [00:00&lt;00:00, 9939.81 examples/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
