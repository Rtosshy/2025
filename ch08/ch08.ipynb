{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1e1YwuFtZd1t",
      "metadata": {
        "editable": true,
        "id": "1e1YwuFtZd1t",
        "tags": []
      },
      "source": [
        "# 第8章: ニューラルネット\n",
        "\n",
        "第7章で取り組んだポジネガ分類を題材として、ニューラルネットワークで分類モデルを実装する。なお、この章ではPyTorchやTensorFlow、JAXなどの深層学習フレームワークを活用せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff",
      "metadata": {
        "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff"
      },
      "source": [
        "## 70. 単語埋め込みの読み込み\n",
        "\n",
        "事前学習済み単語埋め込みを活用し、$|V| \\times d_\\rm{emb}$ の単語埋め込み行列$\\pmb{E}$を作成せよ。ここで、$|V|$は単語埋め込みの語彙数、$d_\\rm{emb}$は単語埋め込みの次元数である。ただし、単語埋め込み行列の先頭の行ベクトル$\\pmb{E}_{0,:}$は、将来的にパディング（`<PAD>`）トークンの埋め込みベクトルとして用いたいので、ゼロベクトルとして予約せよ。ゆえに、$\\pmb{E}$の2行目以降に事前学習済み単語埋め込みを読み込むことになる。\n",
        "\n",
        "もし、Google Newsデータセットの[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ、300次元）を全て読み込んだ場合、$|V|=3000001, d_\\rm{emb}=300$になるはずである（ただ、300万単語の中には、殆ど用いられない稀な単語も含まれるので、語彙を削減した方がメモリの節約になる）。\n",
        "\n",
        "また、単語埋め込み行列の構築と同時に、単語埋め込み行列の各行のインデックス番号（トークンID）と、単語（トークン）への双方向の対応付けを保持せよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "84aaed61",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.11.12 environment at: /home/tosshy/workspace/2025/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m141 packages\u001b[0m \u001b[2min 36ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install -r requirements.txt\n",
        "# numpyのバージョンが変わったらカーネルの再起動が必要らしい"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "tiKdrndSmMyF",
      "metadata": {
        "id": "tiKdrndSmMyF"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "word_vectors = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3a41c8b4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JAX version: 0.6.0\n",
            "Available devices: [CudaDevice(id=0), CudaDevice(id=1)]\n",
            "Default device: gpu\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "print(\"JAX version:\", jax.__version__)\n",
        "print(\"Available devices:\", jax.devices())\n",
        "print(\"Default device:\", jax.default_backend())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "NLlNHe_Pqiph",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "NLlNHe_Pqiph",
        "outputId": "34e112e7-352f-4bb8-a20b-96b2185e7087"
      },
      "outputs": [],
      "source": [
        "embedding_dim = word_vectors.vector_size\n",
        "\n",
        "PAD_TOKEN = '<pad>'\n",
        "PAD_ID = 0\n",
        "\n",
        "word_to_id = {}\n",
        "id_to_word = {}\n",
        "\n",
        "word_to_id[PAD_TOKEN] = PAD_ID\n",
        "id_to_word[PAD_ID] = PAD_TOKEN\n",
        "\n",
        "for i, word in enumerate(word_vectors.index_to_key):\n",
        "    current_id = i + 1\n",
        "    word_to_id[word] = current_id\n",
        "    id_to_word[i] = word\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e548125f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab size: 3000001, embedding dim: 300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating embedding matrix:   0%|                                                                                                                                                                                  | 0/3000001 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating embedding matrix: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3000001/3000001 [00:05<00:00, 517158.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "np_embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float32)\n",
        "\n",
        "print(f\"vocab size: {vocab_size}, embedding dim: {embedding_dim}\")\n",
        "for word, word_id in tqdm(word_to_id.items(), desc=\"Creating embedding matrix\"):\n",
        "    if word == PAD_TOKEN:\n",
        "        continue\n",
        "    np_embedding_matrix[word_id] = word_vectors[word]\n",
        "\n",
        "embedding_matrix = jax.device_put(np_embedding_matrix)\n",
        "\n",
        "# GPUデバイスに配置されているか確認\n",
        "print(f\"Device: {embedding_matrix.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a9702b0c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word: king ID: 6148\n",
            "word: king JAX vector: [ 1.25976562e-01  2.97851562e-02  8.60595703e-03  1.39648438e-01\n",
            " -2.56347656e-02 -3.61328125e-02  1.11816406e-01 -1.98242188e-01\n",
            "  5.12695312e-02  3.63281250e-01 -2.42187500e-01 -3.02734375e-01\n",
            " -1.77734375e-01 -2.49023438e-02 -1.67968750e-01 -1.69921875e-01\n",
            "  3.46679688e-02  5.21850586e-03  4.63867188e-02  1.28906250e-01\n",
            "  1.36718750e-01  1.12792969e-01  5.95703125e-02  1.36718750e-01\n",
            "  1.01074219e-01 -1.76757812e-01 -2.51953125e-01  5.98144531e-02\n",
            "  3.41796875e-01 -3.11279297e-02  1.04492188e-01  6.17675781e-02\n",
            "  1.24511719e-01  4.00390625e-01 -3.22265625e-01  8.39843750e-02\n",
            "  3.90625000e-02  5.85937500e-03  7.03125000e-02  1.72851562e-01\n",
            "  1.38671875e-01 -2.31445312e-01  2.83203125e-01  1.42578125e-01\n",
            "  3.41796875e-01 -2.39257812e-02 -1.09863281e-01  3.32031250e-02\n",
            " -5.46875000e-02  1.53198242e-02 -1.62109375e-01  1.58203125e-01\n",
            " -2.59765625e-01  2.01416016e-02 -1.63085938e-01  1.35803223e-03\n",
            " -1.44531250e-01 -5.68847656e-02  4.29687500e-02 -2.46582031e-02\n",
            "  1.85546875e-01  4.47265625e-01  9.58251953e-03  1.31835938e-01\n",
            "  9.86328125e-02 -1.85546875e-01 -1.00097656e-01 -1.33789062e-01\n",
            " -1.25000000e-01  2.83203125e-01  1.23046875e-01  5.32226562e-02\n",
            " -1.77734375e-01  8.59375000e-02 -2.18505859e-02  2.05078125e-02\n",
            " -1.39648438e-01  2.51464844e-02  1.38671875e-01 -1.05468750e-01\n",
            "  1.38671875e-01  8.88671875e-02 -7.51953125e-02 -2.13623047e-02\n",
            "  1.72851562e-01  4.63867188e-02 -2.65625000e-01  8.91113281e-03\n",
            "  1.49414062e-01  3.78417969e-02  2.38281250e-01 -1.24511719e-01\n",
            " -2.17773438e-01 -1.81640625e-01  2.97851562e-02  5.71289062e-02\n",
            " -2.89306641e-02  1.24511719e-02  9.66796875e-02 -2.31445312e-01\n",
            "  5.81054688e-02  6.68945312e-02  7.08007812e-02 -3.08593750e-01\n",
            " -2.14843750e-01  1.45507812e-01 -4.27734375e-01 -9.39941406e-03\n",
            "  1.54296875e-01 -7.66601562e-02  2.89062500e-01  2.77343750e-01\n",
            " -4.86373901e-04 -1.36718750e-01  3.24218750e-01 -2.46093750e-01\n",
            " -3.03649902e-03 -2.11914062e-01  1.25000000e-01  2.69531250e-01\n",
            "  2.04101562e-01  8.25195312e-02 -2.01171875e-01 -1.60156250e-01\n",
            " -3.78417969e-02 -1.20117188e-01  1.15234375e-01 -4.10156250e-02\n",
            " -3.95507812e-02 -8.98437500e-02  6.34765625e-03  2.03125000e-01\n",
            "  1.86523438e-01  2.73437500e-01  6.29882812e-02  1.41601562e-01\n",
            " -9.81445312e-02  1.38671875e-01  1.82617188e-01  1.73828125e-01\n",
            "  1.73828125e-01 -2.37304688e-01  1.78710938e-01  6.34765625e-02\n",
            "  2.36328125e-01 -2.08984375e-01  8.74023438e-02 -1.66015625e-01\n",
            " -7.91015625e-02  2.43164062e-01 -8.88671875e-02  1.26953125e-01\n",
            " -2.16796875e-01 -1.73828125e-01 -3.59375000e-01 -8.25195312e-02\n",
            " -6.49414062e-02  5.07812500e-02  1.35742188e-01 -7.47070312e-02\n",
            " -1.64062500e-01  1.15356445e-02  4.45312500e-01 -2.15820312e-01\n",
            " -1.11328125e-01 -1.92382812e-01  1.70898438e-01 -1.25000000e-01\n",
            "  2.65502930e-03  1.92382812e-01 -1.74804688e-01  1.39648438e-01\n",
            "  2.92968750e-01  1.13281250e-01  5.95703125e-02 -6.39648438e-02\n",
            "  9.96093750e-02 -2.72216797e-02  1.96533203e-02  4.27246094e-02\n",
            " -2.46093750e-01  6.39648438e-02 -2.25585938e-01 -1.68945312e-01\n",
            "  2.89916992e-03  8.20312500e-02  3.41796875e-01  4.32128906e-02\n",
            "  1.32812500e-01  1.42578125e-01  7.61718750e-02  5.98144531e-02\n",
            " -1.19140625e-01  2.74658203e-03 -6.29882812e-02 -2.72216797e-02\n",
            " -4.82177734e-03 -8.20312500e-02 -2.49023438e-02 -4.00390625e-01\n",
            " -1.06933594e-01  4.24804688e-02  7.76367188e-02 -1.16699219e-01\n",
            "  7.37304688e-02 -9.22851562e-02  1.07910156e-01  1.58203125e-01\n",
            "  4.24804688e-02  1.26953125e-01  3.61328125e-02  2.67578125e-01\n",
            " -1.01074219e-01 -3.02734375e-01 -5.76171875e-02  5.05371094e-02\n",
            "  5.26428223e-04 -2.07031250e-01 -1.38671875e-01 -8.97216797e-03\n",
            " -2.78320312e-02 -1.41601562e-01  2.07031250e-01 -1.58203125e-01\n",
            "  1.27929688e-01  1.49414062e-01 -2.24609375e-02 -8.44726562e-02\n",
            "  1.22558594e-01  2.15820312e-01 -2.13867188e-01 -3.12500000e-01\n",
            " -3.73046875e-01  4.08935547e-03  1.07421875e-01  1.06933594e-01\n",
            "  7.32421875e-02  8.97216797e-03 -3.88183594e-02 -1.29882812e-01\n",
            "  1.49414062e-01 -2.14843750e-01 -1.83868408e-03  9.91210938e-02\n",
            "  1.57226562e-01 -1.14257812e-01 -2.05078125e-01  9.91210938e-02\n",
            "  3.69140625e-01 -1.97265625e-01  3.54003906e-02  1.09375000e-01\n",
            "  1.31835938e-01  1.66992188e-01  2.35351562e-01  1.04980469e-01\n",
            " -4.96093750e-01 -1.64062500e-01 -1.56250000e-01 -5.22460938e-02\n",
            "  1.03027344e-01  2.43164062e-01 -1.88476562e-01  5.07812500e-02\n",
            " -9.37500000e-02 -6.68945312e-02  2.27050781e-02  7.61718750e-02\n",
            "  2.89062500e-01  3.10546875e-01 -5.37109375e-02  2.28515625e-01\n",
            "  2.51464844e-02  6.78710938e-02 -1.21093750e-01 -2.15820312e-01\n",
            " -2.73437500e-01 -3.07617188e-02 -3.37890625e-01  1.53320312e-01\n",
            "  2.33398438e-01 -2.08007812e-01  3.73046875e-01  8.20312500e-02\n",
            "  2.51953125e-01 -7.61718750e-02 -4.66308594e-02 -2.23388672e-02\n",
            "  2.99072266e-02 -5.93261719e-02 -4.66918945e-03 -2.44140625e-01\n",
            " -2.09960938e-01 -2.87109375e-01 -4.54101562e-02 -1.77734375e-01\n",
            " -2.79296875e-01 -8.59375000e-02  9.13085938e-02  2.51953125e-01]\n"
          ]
        }
      ],
      "source": [
        "example = 'king'\n",
        "if example in word_to_id:\n",
        "    example_id = word_to_id[example]\n",
        "    print(f'word: {example} ID: {example_id}')\n",
        "    print(f'word: {example} JAX vector: {embedding_matrix[example_id]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e",
      "metadata": {
        "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e"
      },
      "source": [
        "## 71. データセットの読み込み\n",
        "\n",
        "[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されている[Stanford Sentiment Treebank (SST)](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) をダウンロードし、訓練セット（train.tsv）と開発セット（dev.tsv）のテキストと極性ラベルと読み込み、全てのテキストをトークンID列に変換せよ。このとき、単語埋め込みの語彙でカバーされていない単語は無視し、トークン列に含めないことにせよ。また、テキストの全トークンが単語埋め込みの語彙に含まれておらず、空のトークン列となってしまう事例は、訓練セットおよび開発セットから削除せよ（このため、第7章の実験で得られた正解率と比較できなくなることに注意せよ）。\n",
        "\n",
        "事例の表現方法は任意でよいが、例えば\"contains no wit , only labored gags\"がネガティブに分類される事例は、次のような辞書オブジェクトで表現すればよい。\n",
        "\n",
        "```\n",
        "{'text': 'contains no wit , only labored gags',\n",
        " 'label': tensor([0.]),\n",
        " 'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])}\n",
        "```\n",
        "\n",
        "この例では、`text`はテキスト、`label`は分類ラベル（ポジティブなら`tensor([1.])`、ネガティブなら`tensor([0.])`）、`input_ids`はテキストのトークン列をID列で表現している。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "43fecb0e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hide new secretions from the parental units</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>contains no wit , only labored gags</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>that loves its characters and communicates som...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>remains utterly satisfied to remain the same t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67344</th>\n",
              "      <td>a delightful comedy</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67345</th>\n",
              "      <td>anguish , anger and frustration</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67346</th>\n",
              "      <td>at achieving the modest , crowd-pleasing goals...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67347</th>\n",
              "      <td>a patient viewer</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67348</th>\n",
              "      <td>this new jangle of noise , mayhem and stupidit...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>67349 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                sentence  label\n",
              "0           hide new secretions from the parental units       0\n",
              "1                   contains no wit , only labored gags       0\n",
              "2      that loves its characters and communicates som...      1\n",
              "3      remains utterly satisfied to remain the same t...      0\n",
              "4      on the worst revenge-of-the-nerds clichés the ...      0\n",
              "...                                                  ...    ...\n",
              "67344                               a delightful comedy       1\n",
              "67345                   anguish , anger and frustration       0\n",
              "67346  at achieving the modest , crowd-pleasing goals...      1\n",
              "67347                                  a patient viewer       1\n",
              "67348  this new jangle of noise , mayhem and stupidit...      0\n",
              "\n",
              "[67349 rows x 2 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_path = './data/SST-2/train.tsv'\n",
        "dev_path = './data/SST-2/dev.tsv'\n",
        "\n",
        "train_df = pd.read_csv(train_path, sep='\\t')\n",
        "dev_df = pd.read_csv(dev_path, sep='\\t')\n",
        "\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "d3f12840",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing train data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 67349/67349 [00:18<00:00, 3687.74it/s]\n",
            "Processing dev data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 872/872 [00:00<00:00, 3694.49it/s]\n"
          ]
        }
      ],
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "def text_to_token_ids(text, word_to_id):\n",
        "    words = text.lower().split()\n",
        "\n",
        "    token_ids = [word_to_id[word] for word in words if word in word_to_id]\n",
        "    return token_ids\n",
        "\n",
        "train_data = []\n",
        "\n",
        "for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Processing train data\"):\n",
        "    text = row['sentence']\n",
        "    label = jnp.array(row['label'], dtype=jnp.int32)\n",
        "\n",
        "    token_ids = text_to_token_ids(text, word_to_id)\n",
        "\n",
        "    train_data.append({\n",
        "        'text': text,\n",
        "        'label': label,\n",
        "        'input_ids': token_ids\n",
        "    })\n",
        "\n",
        "dev_data = []\n",
        "\n",
        "for _, row in tqdm(dev_df.iterrows(), total=len(dev_df), desc=\"Processing dev data\"):\n",
        "    text = row['sentence']\n",
        "    label = jnp.array(row['label'], dtype=jnp.int32)\n",
        "\n",
        "    token_ids = text_to_token_ids(text, word_to_id)\n",
        "\n",
        "    dev_data.append({\n",
        "        'text': text,\n",
        "        'label': label,\n",
        "        'input_ids': token_ids\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "39a13481",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'hide new secretions from the parental units ',\n",
              " 'label': Array(0, dtype=int32),\n",
              " 'input_ids': [5785, 66, 113845, 18, 12, 15095, 1594]}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca",
      "metadata": {
        "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca"
      },
      "source": [
        "## 72. Bag of wordsモデルの構築\n",
        "\n",
        "単語埋め込みの平均ベクトルでテキストの特徴ベクトルを表現し、重みベクトルとの内積でポジティブ及びネガティブを分類するニューラルネットワーク（ロジスティック回帰モデル）を設計せよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "6217d96e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating features and labels:   0%|                                                                                                                                                                                 | 0/67349 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating features and labels: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 67349/67349 [01:17<00:00, 869.35it/s]\n",
            "Creating features and labels: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 872/872 [00:01<00:00, 798.87it/s]\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from tqdm import tqdm\n",
        "\n",
        "def create_features_labels(data):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for sample in tqdm(data, desc='Creating features and labels'):\n",
        "        input_ids = sample['input_ids']\n",
        "        label = sample['label']\n",
        "\n",
        "        if not input_ids:\n",
        "            continue\n",
        "\n",
        "        input_ids_array = jnp.array(input_ids)\n",
        "\n",
        "        token_embbedings = embedding_matrix[input_ids_array]\n",
        "\n",
        "        sentence_feature = token_embbedings.mean(axis=0)\n",
        "\n",
        "        features.append(sentence_feature)\n",
        "        labels.append(label)\n",
        "\n",
        "    features = jnp.array(features)\n",
        "    labels = jnp.array(labels)\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "train_features, train_labels = create_features_labels(train_data)\n",
        "dev_features, dev_labels = create_features_labels(dev_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "85e6900e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(66650, 300)\n",
            "(66650,)\n",
            "(872, 300)\n",
            "(872,)\n"
          ]
        }
      ],
      "source": [
        "print(train_features.shape)\n",
        "print(train_labels.shape)\n",
        "print(dev_features.shape)\n",
        "print(dev_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "a842bee7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import flax.linen as nn\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Dense(features=1)(x)\n",
        "        return jax.nn.sigmoid(x.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72385c44-ceab-4d62-a4df-3023e15a37e2",
      "metadata": {
        "id": "72385c44-ceab-4d62-a4df-3023e15a37e2"
      },
      "source": [
        "## 73. モデルの学習\n",
        "\n",
        "問題72で設計したモデルの重みベクトルを訓練セット上で学習せよ。ただし、学習中は単語埋め込み行列の値を固定せよ（単語埋め込み行列のファインチューニングは行わない）。また、学習時に損失値を表示するなど、学習の進捗状況をモニタリングできるようにせよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "2401dc4c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Dense_0': {'bias': (1,), 'kernel': (300, 1)}}\n"
          ]
        }
      ],
      "source": [
        "model = LogisticRegression()\n",
        "key = jax.random.PRNGKey(0)\n",
        "dummy_x = train_features[0:1]\n",
        "\n",
        "params = model.init(key, dummy_x)['params']\n",
        "print(jax.tree_util.tree_map(lambda x: x.shape, params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "d038319b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn(params, x_batch, y_batch):\n",
        "    predictions = model.apply({'params': params}, x_batch)\n",
        "    predictions = jnp.clip(predictions, 1e-7, 1 - 1e-7)\n",
        "    log_likelihood = y_batch * jnp.log(predictions) + (1 - y_batch) * jnp.log(1 - predictions)\n",
        "    return -jnp.mean(log_likelihood)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f7bd11e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import optax\n",
        "\n",
        "@jax.jit\n",
        "def train_step(params, opt_state, x_batch, y_batch):\n",
        "    loss_value, grads = jax.value_and_grad(lambda p: loss_fn(p, x_batch, y_batch))(params)\n",
        "\n",
        "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, opt_state, loss_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "e2e7f8ac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10, Loss: 0.6129656434059143\n",
            "Epoch: 20, Loss: 0.5527616143226624\n",
            "Epoch: 30, Loss: 0.5122967958450317\n",
            "Epoch: 40, Loss: 0.4848070740699768\n",
            "Epoch: 50, Loss: 0.46541622281074524\n",
            "Epoch: 60, Loss: 0.4511268138885498\n",
            "Epoch: 70, Loss: 0.44015443325042725\n",
            "Epoch: 80, Loss: 0.4314348101615906\n",
            "Epoch: 90, Loss: 0.42431604862213135\n",
            "Epoch: 100, Loss: 0.4183818995952606\n"
          ]
        }
      ],
      "source": [
        "lr = 0.01\n",
        "optimizer = optax.adam(lr)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "n_epochs = 100\n",
        "\n",
        "train_labels = train_labels.astype(jnp.float32)\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    params, opt_state, current_loss = train_step(params, opt_state, train_features, train_labels)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch: {epoch}, Loss: {current_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f",
      "metadata": {
        "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f"
      },
      "source": [
        "## 74. モデルの評価\n",
        "\n",
        "問題73で学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "076ec608",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
            "(872,)\n"
          ]
        }
      ],
      "source": [
        "predictions = model.apply({'params': params}, dev_features)\n",
        "print(predictions.shape)\n",
        "print(dev_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "bfd1db51",
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_labels = (predictions > 0.5).astype(jnp.int32)\n",
        "true_labels = dev_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "d502329d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.7775229357798165\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(pred_labels, true_labels)\n",
        "print('accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O08V9g0mcJwe",
      "metadata": {
        "id": "O08V9g0mcJwe"
      },
      "source": [
        "## 75. パディング\n",
        "\n",
        "複数の事例が与えられたとき、これらをまとめて一つのテンソル・オブジェクトで表現する関数`collate`を実装せよ。与えられた複数の事例のトークン列の長さが異なるときは、トークン列の長さが最も長いものに揃え、0番のトークンIDでパディングをせよ。さらに、トークン列の長さが長いものから順に、事例を並び替えよ。\n",
        "\n",
        "例えば、訓練データセットの冒頭の4事例が次のように表されているとき、\n",
        "\n",
        "```\n",
        "[{'text': 'hide new secretions from the parental units',\n",
        "  'label': tensor([0.]),\n",
        "  'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])},\n",
        " {'text': 'contains no wit , only labored gags',\n",
        "  'label': tensor([0.]),\n",
        "  'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])},\n",
        " {'text': 'that loves its characters and communicates something rather beautiful about human nature',\n",
        "  'label': tensor([1.]),\n",
        "  'input_ids': tensor([    4,  5053,    45,  3305, 31647,   348,   904,  2815,    47,  1276,  1964])},\n",
        " {'text': 'remains utterly satisfied to remain the same throughout',\n",
        "  'label': tensor([0.]),\n",
        "  'input_ids': tensor([  987, 14528,  4941,   873,    12,   208,   898])}]\n",
        "```\n",
        "\n",
        "`collate`関数を通した結果は以下のようになることが想定される。\n",
        "\n",
        "```\n",
        "{'input_ids': tensor([\n",
        "    [     4,   5053,     45,   3305,  31647,    348,    904,   2815,     47,   1276,   1964],\n",
        "    [  5785,     66, 113845,     18,     12,  15095,   1594,      0,      0,      0,      0],\n",
        "    [   987,  14528,   4941,    873,     12,    208,    898,      0,      0,      0,      0],\n",
        "    [  3475,     87,  15888,     90,  27695,  42637,      0,      0,      0,      0,      0]]),\n",
        " 'label': tensor([\n",
        "    [1.],\n",
        "    [0.],\n",
        "    [0.],\n",
        "    [0.]])}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "0e2c576d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate(data, max_length, pad_id=PAD_ID):\n",
        "    sorted_data = sorted(data, key=lambda sample: len(sample['text']), reverse=True)\n",
        "\n",
        "    collated_data = []\n",
        "\n",
        "    for sample in tqdm(sorted_data):\n",
        "        text = sample['text']\n",
        "        input_ids = sample['input_ids']\n",
        "        label = sample['label']\n",
        "\n",
        "        current_length = len(input_ids)\n",
        "\n",
        "        if current_length < max_length:\n",
        "            num_padding = max_length - current_length\n",
        "            padded_ids = input_ids + [pad_id] * num_padding\n",
        "        else:\n",
        "            padded_ids = input_ids[:max_length]\n",
        "        \n",
        "        padded_ids = jnp.array(padded_ids, dtype=jnp.int32)\n",
        "\n",
        "        collated_data.append({\n",
        "            'text': text,\n",
        "            'input_ids': padded_ids,\n",
        "            'label': label\n",
        "        })\n",
        "    \n",
        "    return collated_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "1fb4aacd",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 67349/67349 [00:19<00:00, 3534.61it/s]\n",
            "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 872/872 [00:00<00:00, 3399.65it/s]\n"
          ]
        }
      ],
      "source": [
        "collated_train_data = collate(train_data, max_length=270)\n",
        "collated_dev_data = collate(dev_data, max_length=270)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9NzvuZ-5ebDU",
      "metadata": {
        "id": "9NzvuZ-5ebDU"
      },
      "source": [
        "## 76. ミニバッチ学習\n",
        "\n",
        "問題75のパディングの処理を活用して、ミニバッチでモデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RUbjivUTejxn",
      "metadata": {
        "id": "RUbjivUTejxn"
      },
      "source": [
        "## 77. GPU上での学習\n",
        "\n",
        "問題76のモデル学習をGPU上で実行せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZUY1PsD-eplq",
      "metadata": {
        "id": "ZUY1PsD-eplq"
      },
      "source": [
        "## 78. 単語埋め込みのファインチューニング\n",
        "\n",
        "問題77の学習において、単語埋め込みのパラメータも同時に更新するファインチューニングを導入せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jVAdWIq0evKR",
      "metadata": {
        "id": "jVAdWIq0evKR"
      },
      "source": [
        "## 79. アーキテクチャの変更\n",
        "\n",
        "ニューラルネットワークのアーキテクチャを自由に変更し、モデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。例えば、テキストの特徴ベクトル（単語埋め込みの平均ベクトル）に対して多層のニューラルネットワークを通したり、畳み込みニューラルネットワーク（CNN; Convolutional Neural Network）や再帰型ニューラルネットワーク（RNN; Recurrent Neural Network）などのモデルの学習に挑戦するとよい。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
