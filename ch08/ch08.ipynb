{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1e1YwuFtZd1t",
      "metadata": {
        "editable": true,
        "id": "1e1YwuFtZd1t",
        "tags": []
      },
      "source": [
        "# 第8章: ニューラルネット\n",
        "\n",
        "第7章で取り組んだポジネガ分類を題材として、ニューラルネットワークで分類モデルを実装する。なお、この章ではPyTorchやTensorFlow、JAXなどの深層学習フレームワークを活用せよ。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff",
      "metadata": {
        "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff"
      },
      "source": [
        "## 70. 単語埋め込みの読み込み\n",
        "\n",
        "事前学習済み単語埋め込みを活用し、$|V| \\times d_\\rm{emb}$ の単語埋め込み行列$\\pmb{E}$を作成せよ。ここで、$|V|$は単語埋め込みの語彙数、$d_\\rm{emb}$は単語埋め込みの次元数である。ただし、単語埋め込み行列の先頭の行ベクトル$\\pmb{E}_{0,:}$は、将来的にパディング（`<PAD>`）トークンの埋め込みベクトルとして用いたいので、ゼロベクトルとして予約せよ。ゆえに、$\\pmb{E}$の2行目以降に事前学習済み単語埋め込みを読み込むことになる。\n",
        "\n",
        "もし、Google Newsデータセットの[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ、300次元）を全て読み込んだ場合、$|V|=3000001, d_\\rm{emb}=300$になるはずである（ただ、300万単語の中には、殆ど用いられない稀な単語も含まれるので、語彙を削減した方がメモリの節約になる）。\n",
        "\n",
        "また、単語埋め込み行列の構築と同時に、単語埋め込み行列の各行のインデックス番号（トークンID）と、単語（トークン）への双方向の対応付けを保持せよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "84aaed61",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.11.12 environment at: /home/tosshy/workspace/2025/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m141 packages\u001b[0m \u001b[2min 34ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install -r requirements.txt\n",
        "# numpyのバージョンが変わったらカーネルの再起動が必要らしい"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "tiKdrndSmMyF",
      "metadata": {
        "id": "tiKdrndSmMyF"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "word_vectors = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9a6a47ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "jax.config.update('jax_platform_name', 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3a41c8b4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JAX version: 0.6.0\n",
            "Available devices: [CpuDevice(id=0)]\n",
            "Default device: cpu\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "print(\"JAX version:\", jax.__version__)\n",
        "print(\"Available devices:\", jax.devices())\n",
        "print(\"Default device:\", jax.default_backend())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "NLlNHe_Pqiph",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "NLlNHe_Pqiph",
        "outputId": "34e112e7-352f-4bb8-a20b-96b2185e7087"
      },
      "outputs": [],
      "source": [
        "embedding_dim = word_vectors.vector_size\n",
        "\n",
        "PAD_TOKEN = '<pad>'\n",
        "PAD_ID = 0\n",
        "\n",
        "word_to_id = {}\n",
        "id_to_word = {}\n",
        "\n",
        "word_to_id[PAD_TOKEN] = PAD_ID\n",
        "id_to_word[PAD_ID] = PAD_TOKEN\n",
        "\n",
        "for i, word in enumerate(word_vectors.index_to_key):\n",
        "    current_id = i + 1\n",
        "    word_to_id[word] = current_id\n",
        "    id_to_word[i] = word\n",
        "\n",
        "vocab_size = len(word_to_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "e548125f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab size: 3000001, embedding dim: 300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating embedding matrix: 100%|█████████████████████████████████████████████████████████████| 3000001/3000001 [00:08<00:00, 374220.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: TFRT_CPU_0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "np_embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float16)\n",
        "\n",
        "print(f\"vocab size: {vocab_size}, embedding dim: {embedding_dim}\")\n",
        "for word, word_id in tqdm(word_to_id.items(), desc=\"Creating embedding matrix\"):\n",
        "    if word == PAD_TOKEN:\n",
        "        continue\n",
        "    np_embedding_matrix[word_id] = word_vectors[word]\n",
        "\n",
        "embedding_matrix = jax.device_put(np_embedding_matrix)\n",
        "\n",
        "# GPUデバイスに配置されているか確認\n",
        "print(f\"Device: {embedding_matrix.device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87e29d80",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: TFRT_CPU_0\n"
          ]
        }
      ],
      "source": [
        "embedding_matrix = jax.device_put(embedding_matrix, jax.devices('cpu')[0])\n",
        "print(f\"Device: {embedding_matrix.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "df910491",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1800000600\n"
          ]
        }
      ],
      "source": [
        "print(embedding_matrix.nbytes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a9702b0c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word: king ID: 6148\n",
            "word: king JAX vector: [ 1.2598e-01  2.9785e-02  8.6060e-03  1.3965e-01 -2.5635e-02 -3.6133e-02\n",
            "  1.1182e-01 -1.9824e-01  5.1270e-02  3.6328e-01 -2.4219e-01 -3.0273e-01\n",
            " -1.7773e-01 -2.4902e-02 -1.6797e-01 -1.6992e-01  3.4668e-02  5.2185e-03\n",
            "  4.6387e-02  1.2891e-01  1.3672e-01  1.1279e-01  5.9570e-02  1.3672e-01\n",
            "  1.0107e-01 -1.7676e-01 -2.5195e-01  5.9814e-02  3.4180e-01 -3.1128e-02\n",
            "  1.0449e-01  6.1768e-02  1.2451e-01  4.0039e-01 -3.2227e-01  8.3984e-02\n",
            "  3.9062e-02  5.8594e-03  7.0312e-02  1.7285e-01  1.3867e-01 -2.3145e-01\n",
            "  2.8320e-01  1.4258e-01  3.4180e-01 -2.3926e-02 -1.0986e-01  3.3203e-02\n",
            " -5.4688e-02  1.5320e-02 -1.6211e-01  1.5820e-01 -2.5977e-01  2.0142e-02\n",
            " -1.6309e-01  1.3580e-03 -1.4453e-01 -5.6885e-02  4.2969e-02 -2.4658e-02\n",
            "  1.8555e-01  4.4727e-01  9.5825e-03  1.3184e-01  9.8633e-02 -1.8555e-01\n",
            " -1.0010e-01 -1.3379e-01 -1.2500e-01  2.8320e-01  1.2305e-01  5.3223e-02\n",
            " -1.7773e-01  8.5938e-02 -2.1851e-02  2.0508e-02 -1.3965e-01  2.5146e-02\n",
            "  1.3867e-01 -1.0547e-01  1.3867e-01  8.8867e-02 -7.5195e-02 -2.1362e-02\n",
            "  1.7285e-01  4.6387e-02 -2.6562e-01  8.9111e-03  1.4941e-01  3.7842e-02\n",
            "  2.3828e-01 -1.2451e-01 -2.1777e-01 -1.8164e-01  2.9785e-02  5.7129e-02\n",
            " -2.8931e-02  1.2451e-02  9.6680e-02 -2.3145e-01  5.8105e-02  6.6895e-02\n",
            "  7.0801e-02 -3.0859e-01 -2.1484e-01  1.4551e-01 -4.2773e-01 -9.3994e-03\n",
            "  1.5430e-01 -7.6660e-02  2.8906e-01  2.7734e-01 -4.8637e-04 -1.3672e-01\n",
            "  3.2422e-01 -2.4609e-01 -3.0365e-03 -2.1191e-01  1.2500e-01  2.6953e-01\n",
            "  2.0410e-01  8.2520e-02 -2.0117e-01 -1.6016e-01 -3.7842e-02 -1.2012e-01\n",
            "  1.1523e-01 -4.1016e-02 -3.9551e-02 -8.9844e-02  6.3477e-03  2.0312e-01\n",
            "  1.8652e-01  2.7344e-01  6.2988e-02  1.4160e-01 -9.8145e-02  1.3867e-01\n",
            "  1.8262e-01  1.7383e-01  1.7383e-01 -2.3730e-01  1.7871e-01  6.3477e-02\n",
            "  2.3633e-01 -2.0898e-01  8.7402e-02 -1.6602e-01 -7.9102e-02  2.4316e-01\n",
            " -8.8867e-02  1.2695e-01 -2.1680e-01 -1.7383e-01 -3.5938e-01 -8.2520e-02\n",
            " -6.4941e-02  5.0781e-02  1.3574e-01 -7.4707e-02 -1.6406e-01  1.1536e-02\n",
            "  4.4531e-01 -2.1582e-01 -1.1133e-01 -1.9238e-01  1.7090e-01 -1.2500e-01\n",
            "  2.6550e-03  1.9238e-01 -1.7480e-01  1.3965e-01  2.9297e-01  1.1328e-01\n",
            "  5.9570e-02 -6.3965e-02  9.9609e-02 -2.7222e-02  1.9653e-02  4.2725e-02\n",
            " -2.4609e-01  6.3965e-02 -2.2559e-01 -1.6895e-01  2.8992e-03  8.2031e-02\n",
            "  3.4180e-01  4.3213e-02  1.3281e-01  1.4258e-01  7.6172e-02  5.9814e-02\n",
            " -1.1914e-01  2.7466e-03 -6.2988e-02 -2.7222e-02 -4.8218e-03 -8.2031e-02\n",
            " -2.4902e-02 -4.0039e-01 -1.0693e-01  4.2480e-02  7.7637e-02 -1.1670e-01\n",
            "  7.3730e-02 -9.2285e-02  1.0791e-01  1.5820e-01  4.2480e-02  1.2695e-01\n",
            "  3.6133e-02  2.6758e-01 -1.0107e-01 -3.0273e-01 -5.7617e-02  5.0537e-02\n",
            "  5.2643e-04 -2.0703e-01 -1.3867e-01 -8.9722e-03 -2.7832e-02 -1.4160e-01\n",
            "  2.0703e-01 -1.5820e-01  1.2793e-01  1.4941e-01 -2.2461e-02 -8.4473e-02\n",
            "  1.2256e-01  2.1582e-01 -2.1387e-01 -3.1250e-01 -3.7305e-01  4.0894e-03\n",
            "  1.0742e-01  1.0693e-01  7.3242e-02  8.9722e-03 -3.8818e-02 -1.2988e-01\n",
            "  1.4941e-01 -2.1484e-01 -1.8387e-03  9.9121e-02  1.5723e-01 -1.1426e-01\n",
            " -2.0508e-01  9.9121e-02  3.6914e-01 -1.9727e-01  3.5400e-02  1.0938e-01\n",
            "  1.3184e-01  1.6699e-01  2.3535e-01  1.0498e-01 -4.9609e-01 -1.6406e-01\n",
            " -1.5625e-01 -5.2246e-02  1.0303e-01  2.4316e-01 -1.8848e-01  5.0781e-02\n",
            " -9.3750e-02 -6.6895e-02  2.2705e-02  7.6172e-02  2.8906e-01  3.1055e-01\n",
            " -5.3711e-02  2.2852e-01  2.5146e-02  6.7871e-02 -1.2109e-01 -2.1582e-01\n",
            " -2.7344e-01 -3.0762e-02 -3.3789e-01  1.5332e-01  2.3340e-01 -2.0801e-01\n",
            "  3.7305e-01  8.2031e-02  2.5195e-01 -7.6172e-02 -4.6631e-02 -2.2339e-02\n",
            "  2.9907e-02 -5.9326e-02 -4.6692e-03 -2.4414e-01 -2.0996e-01 -2.8711e-01\n",
            " -4.5410e-02 -1.7773e-01 -2.7930e-01 -8.5938e-02  9.1309e-02  2.5195e-01]\n"
          ]
        }
      ],
      "source": [
        "example = 'king'\n",
        "if example in word_to_id:\n",
        "    example_id = word_to_id[example]\n",
        "    print(f'word: {example} ID: {example_id}')\n",
        "    print(f'word: {example} JAX vector: {embedding_matrix[example_id]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e",
      "metadata": {
        "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e"
      },
      "source": [
        "## 71. データセットの読み込み\n",
        "\n",
        "[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されている[Stanford Sentiment Treebank (SST)](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) をダウンロードし、訓練セット（train.tsv）と開発セット（dev.tsv）のテキストと極性ラベルと読み込み、全てのテキストをトークンID列に変換せよ。このとき、単語埋め込みの語彙でカバーされていない単語は無視し、トークン列に含めないことにせよ。また、テキストの全トークンが単語埋め込みの語彙に含まれておらず、空のトークン列となってしまう事例は、訓練セットおよび開発セットから削除せよ（このため、第7章の実験で得られた正解率と比較できなくなることに注意せよ）。\n",
        "\n",
        "事例の表現方法は任意でよいが、例えば\"contains no wit , only labored gags\"がネガティブに分類される事例は、次のような辞書オブジェクトで表現すればよい。\n",
        "\n",
        "```\n",
        "{'text': 'contains no wit , only labored gags',\n",
        " 'label': tensor([0.]),\n",
        " 'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])}\n",
        "```\n",
        "\n",
        "この例では、`text`はテキスト、`label`は分類ラベル（ポジティブなら`tensor([1.])`、ネガティブなら`tensor([0.])`）、`input_ids`はテキストのトークン列をID列で表現している。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "43fecb0e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hide new secretions from the parental units</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>contains no wit , only labored gags</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>that loves its characters and communicates som...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>remains utterly satisfied to remain the same t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67344</th>\n",
              "      <td>a delightful comedy</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67345</th>\n",
              "      <td>anguish , anger and frustration</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67346</th>\n",
              "      <td>at achieving the modest , crowd-pleasing goals...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67347</th>\n",
              "      <td>a patient viewer</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67348</th>\n",
              "      <td>this new jangle of noise , mayhem and stupidit...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>67349 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                sentence  label\n",
              "0           hide new secretions from the parental units       0\n",
              "1                   contains no wit , only labored gags       0\n",
              "2      that loves its characters and communicates som...      1\n",
              "3      remains utterly satisfied to remain the same t...      0\n",
              "4      on the worst revenge-of-the-nerds clichés the ...      0\n",
              "...                                                  ...    ...\n",
              "67344                               a delightful comedy       1\n",
              "67345                   anguish , anger and frustration       0\n",
              "67346  at achieving the modest , crowd-pleasing goals...      1\n",
              "67347                                  a patient viewer       1\n",
              "67348  this new jangle of noise , mayhem and stupidit...      0\n",
              "\n",
              "[67349 rows x 2 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_path = './data/SST-2/train.tsv'\n",
        "dev_path = './data/SST-2/dev.tsv'\n",
        "\n",
        "train_df = pd.read_csv(train_path, sep='\\t')\n",
        "dev_df = pd.read_csv(dev_path, sep='\\t')\n",
        "\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "d3f12840",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing train data:   1%|▌                                                                        | 546/67349 [00:00<00:12, 5454.44it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing train data: 100%|███████████████████████████████████████████████████████████████████████| 67349/67349 [00:07<00:00, 9126.79it/s]\n",
            "Processing dev data: 100%|█████████████████████████████████████████████████████████████████████████████| 872/872 [00:00<00:00, 8627.47it/s]\n"
          ]
        }
      ],
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "def text_to_token_ids(text, word_to_id):\n",
        "    words = text.lower().split()\n",
        "\n",
        "    token_ids = [word_to_id[word] for word in words if word in word_to_id]\n",
        "    return token_ids\n",
        "\n",
        "train_data = []\n",
        "\n",
        "for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Processing train data\"):\n",
        "    text = row['sentence']\n",
        "    label = jnp.array(row['label'], dtype=jnp.int8)\n",
        "\n",
        "    token_ids = text_to_token_ids(text, word_to_id)\n",
        "\n",
        "    if token_ids:\n",
        "        train_data.append({\n",
        "            'text': text,\n",
        "            'label': label,\n",
        "            'input_ids': token_ids\n",
        "        })\n",
        "\n",
        "dev_data = []\n",
        "\n",
        "for _, row in tqdm(dev_df.iterrows(), total=len(dev_df), desc=\"Processing dev data\"):\n",
        "    text = row['sentence']\n",
        "    label = jnp.array(row['label'], dtype=jnp.int8)\n",
        "\n",
        "    token_ids = text_to_token_ids(text, word_to_id)\n",
        "\n",
        "    if token_ids:\n",
        "        dev_data.append({\n",
        "            'text': text,\n",
        "            'label': label,\n",
        "            'input_ids': token_ids\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "53e7f278",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "562488\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.getsizeof(train_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "39a13481",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': 'hide new secretions from the parental units ',\n",
              " 'label': Array(0, dtype=int8),\n",
              " 'input_ids': [5785, 66, 113845, 18, 12, 15095, 1594]}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca",
      "metadata": {
        "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca"
      },
      "source": [
        "## 72. Bag of wordsモデルの構築\n",
        "\n",
        "単語埋め込みの平均ベクトルでテキストの特徴ベクトルを表現し、重みベクトルとの内積でポジティブ及びネガティブを分類するニューラルネットワーク（ロジスティック回帰モデル）を設計せよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6217d96e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from tqdm import tqdm\n",
        "\n",
        "def create_features_labels(data):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for sample in tqdm(data, desc='Creating features and labels'):\n",
        "        input_ids = sample['input_ids']\n",
        "        label = sample['label']\n",
        "\n",
        "        input_ids_array = jnp.array(input_ids)\n",
        "\n",
        "        token_embbedings = embedding_matrix[input_ids_array]\n",
        "\n",
        "        sentence_feature = token_embbedings.mean(axis=0)\n",
        "\n",
        "        features.append(sentence_feature)\n",
        "        labels.append(label)\n",
        "\n",
        "    features = jnp.array(features)\n",
        "    labels = jnp.array(labels)\n",
        "\n",
        "    return features, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "467e44c6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating features and labels: 100%|████████████████████████████████████████████████████████████████| 66650/66650 [00:45<00:00, 1466.20it/s]\n",
            "Creating features and labels: 100%|████████████████████████████████████████████████████████████████████| 872/872 [00:00<00:00, 1363.00it/s]\n"
          ]
        }
      ],
      "source": [
        "train_features, train_labels = create_features_labels(train_data)\n",
        "dev_features, dev_labels = create_features_labels(dev_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "85e6900e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(66650, 300)\n",
            "(66650,)\n",
            "(872, 300)\n",
            "(872,)\n"
          ]
        }
      ],
      "source": [
        "print(train_features.shape)\n",
        "print(train_labels.shape)\n",
        "print(dev_features.shape)\n",
        "print(dev_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a842bee7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import flax.linen as nn\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Dense(features=1)(x)\n",
        "        return jax.nn.sigmoid(x.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72385c44-ceab-4d62-a4df-3023e15a37e2",
      "metadata": {
        "id": "72385c44-ceab-4d62-a4df-3023e15a37e2"
      },
      "source": [
        "## 73. モデルの学習\n",
        "\n",
        "問題72で設計したモデルの重みベクトルを訓練セット上で学習せよ。ただし、学習中は単語埋め込み行列の値を固定せよ（単語埋め込み行列のファインチューニングは行わない）。また、学習時に損失値を表示するなど、学習の進捗状況をモニタリングできるようにせよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2401dc4c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Dense_0': {'bias': (1,), 'kernel': (300, 1)}}\n"
          ]
        }
      ],
      "source": [
        "model = LogisticRegression()\n",
        "key = jax.random.PRNGKey(0) # 乱数シードもしっかりやっておく\n",
        "dummy_x = train_features[0:1]\n",
        "\n",
        "params = model.init(key, dummy_x)['params']\n",
        "print(jax.tree_util.tree_map(lambda x: x.shape, params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d038319b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def loss_fn(params, x_batch, y_batch):\n",
        "    predictions = model.apply({'params': params}, x_batch)\n",
        "    predictions = jnp.clip(predictions, 1e-7, 1 - 1e-7)\n",
        "    log_likelihood = y_batch * jnp.log(predictions) + (1 - y_batch) * jnp.log(1 - predictions)\n",
        "    return -jnp.mean(log_likelihood)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "f7bd11e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import optax\n",
        "\n",
        "@jax.jit\n",
        "def train_step(params, opt_state, x_batch, y_batch):\n",
        "    loss_value, grads = jax.value_and_grad(lambda p: loss_fn(p, x_batch, y_batch))(params)\n",
        "\n",
        "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, opt_state, loss_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e2e7f8ac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10, Loss: 0.4317786693572998\n",
            "Epoch: 20, Loss: 0.3934055268764496\n",
            "Epoch: 30, Loss: 0.38222143054008484\n",
            "Epoch: 40, Loss: 0.3761337399482727\n",
            "Epoch: 50, Loss: 0.3727072477340698\n",
            "Epoch: 60, Loss: 0.3706419765949249\n",
            "Epoch: 70, Loss: 0.3692324161529541\n",
            "Epoch: 80, Loss: 0.3682350218296051\n",
            "Epoch: 90, Loss: 0.36749252676963806\n",
            "Epoch: 100, Loss: 0.36693626642227173\n"
          ]
        }
      ],
      "source": [
        "lr = 0.1\n",
        "optimizer = optax.adam(lr)\n",
        "opt_state = optimizer.init(params)\n",
        "\n",
        "n_epochs = 100\n",
        "\n",
        "train_labels = train_labels.astype(jnp.float32)\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    params, opt_state, current_loss = train_step(params, opt_state, train_features, train_labels)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch: {epoch}, Loss: {current_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f",
      "metadata": {
        "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f"
      },
      "source": [
        "## 74. モデルの評価\n",
        "\n",
        "問題73で学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "076ec608",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(872,)\n",
            "(872,)\n"
          ]
        }
      ],
      "source": [
        "predictions = model.apply({'params': params}, dev_features)\n",
        "print(predictions.shape)\n",
        "print(dev_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "bfd1db51",
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_labels = (predictions > 0.5).astype(jnp.int32)\n",
        "true_labels = dev_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d502329d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.801605504587156\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(pred_labels, true_labels)\n",
        "print('accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O08V9g0mcJwe",
      "metadata": {
        "id": "O08V9g0mcJwe"
      },
      "source": [
        "## 75. パディング\n",
        "\n",
        "複数の事例が与えられたとき、これらをまとめて一つのテンソル・オブジェクトで表現する関数`collate`を実装せよ。与えられた複数の事例のトークン列の長さが異なるときは、トークン列の長さが最も長いものに揃え、0番のトークンIDでパディングをせよ。さらに、トークン列の長さが長いものから順に、事例を並び替えよ。\n",
        "\n",
        "例えば、訓練データセットの冒頭の4事例が次のように表されているとき、\n",
        "\n",
        "```\n",
        "[{'text': 'hide new secretions from the parental units',\n",
        "  'label': tensor([0.]),\n",
        "  'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])},\n",
        " {'text': 'contains no wit , only labored gags',\n",
        "  'label': tensor([0.]),\n",
        "  'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])},\n",
        " {'text': 'that loves its characters and communicates something rather beautiful about human nature',\n",
        "  'label': tensor([1.]),\n",
        "  'input_ids': tensor([    4,  5053,    45,  3305, 31647,   348,   904,  2815,    47,  1276,  1964])},\n",
        " {'text': 'remains utterly satisfied to remain the same throughout',\n",
        "  'label': tensor([0.]),\n",
        "  'input_ids': tensor([  987, 14528,  4941,   873,    12,   208,   898])}]\n",
        "```\n",
        "\n",
        "`collate`関数を通した結果は以下のようになることが想定される。\n",
        "\n",
        "```\n",
        "{'input_ids': tensor([\n",
        "    [     4,   5053,     45,   3305,  31647,    348,    904,   2815,     47,   1276,   1964],\n",
        "    [  5785,     66, 113845,     18,     12,  15095,   1594,      0,      0,      0,      0],\n",
        "    [   987,  14528,   4941,    873,     12,    208,    898,      0,      0,      0,      0],\n",
        "    [  3475,     87,  15888,     90,  27695,  42637,      0,      0,      0,      0,      0]]),\n",
        " 'label': tensor([\n",
        "    [1.],\n",
        "    [0.],\n",
        "    [0.],\n",
        "    [0.]])}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "0e2c576d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate(data, max_length, pad_id=PAD_ID):\n",
        "    sorted_data = sorted(data, key=lambda sample: len(sample['text']), reverse=True)\n",
        "\n",
        "    collated_data = []\n",
        "\n",
        "    for sample in tqdm(sorted_data):\n",
        "        text = sample['text']\n",
        "        input_ids = sample['input_ids']\n",
        "        label = sample['label']\n",
        "\n",
        "        current_length = len(input_ids)\n",
        "\n",
        "        if current_length < max_length:\n",
        "            num_padding = max_length - current_length\n",
        "            padded_ids = input_ids + [pad_id] * num_padding\n",
        "        else:\n",
        "            padded_ids = input_ids[:max_length]\n",
        "        \n",
        "        padded_ids = jnp.array(padded_ids, dtype=jnp.int32)\n",
        "\n",
        "        collated_data.append({\n",
        "            'text': text,\n",
        "            'input_ids': padded_ids,\n",
        "            'label': label\n",
        "        })\n",
        "    \n",
        "    return collated_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "1fb4aacd",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 66650/66650 [00:08<00:00, 7583.98it/s]\n",
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 872/872 [00:00<00:00, 7786.68it/s]\n"
          ]
        }
      ],
      "source": [
        "collated_train_data = collate(train_data, max_length=270)\n",
        "collated_dev_data = collate(dev_data, max_length=270)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9NzvuZ-5ebDU",
      "metadata": {
        "id": "9NzvuZ-5ebDU"
      },
      "source": [
        "## 76. ミニバッチ学習\n",
        "\n",
        "問題75のパディングの処理を活用して、ミニバッチでモデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "70fee33d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating features and labels:   0%|                                                                              | 0/66650 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating features and labels: 100%|████████████████████████████████████████████████████████████████| 66650/66650 [00:43<00:00, 1531.21it/s]\n",
            "Creating features and labels: 100%|████████████████████████████████████████████████████████████████████| 872/872 [00:00<00:00, 1484.48it/s]\n"
          ]
        }
      ],
      "source": [
        "train_features, train_labels = create_features_labels(collated_train_data)\n",
        "dev_features, dev_labels = create_features_labels(collated_dev_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "24a334b2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(66650, 300)\n",
            "(66650,)\n",
            "(872, 300)\n",
            "(872,)\n"
          ]
        }
      ],
      "source": [
        "print(train_features.shape)\n",
        "print(train_labels.shape)\n",
        "print(dev_features.shape)\n",
        "print(dev_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "cf7ee2ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "import flax.linen as nn\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Dense(features=1)(x)\n",
        "        return jax.nn.sigmoid(x.squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b2157604",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Dense_0': {'bias': (1,), 'kernel': (300, 1)}}\n"
          ]
        }
      ],
      "source": [
        "model = LogisticRegression()\n",
        "key = jax.random.PRNGKey(0) # 乱数シードもしっかりやっておく\n",
        "dummy_x = train_features[0:1]\n",
        "\n",
        "params = model.init(key, dummy_x)['params']\n",
        "print(jax.tree_util.tree_map(lambda x: x.shape, params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "fbff30cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from flax.training import train_state\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "import numpy as np\n",
        "\n",
        "class TrainState(train_state.TrainState):\n",
        "    pass\n",
        "\n",
        "tx = optax.adam(learning_rate=0.1)\n",
        "state = TrainState.create(\n",
        "    apply_fn=model.apply,\n",
        "    params=params,\n",
        "    tx=tx,\n",
        ")\n",
        "\n",
        "def data_loader(features, labels, batch_size, rng):\n",
        "    N = features.shape[0]\n",
        "    perm = random.permutation(rng, N)\n",
        "    perm = perm[:(N // batch_size) * batch_size].reshape(-1, batch_size)\n",
        "    for idx in perm:\n",
        "        yield features[idx], labels[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "04cab1c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step(state, x, y):\n",
        "    def loss_fn(params):\n",
        "        preds = state.apply_fn({'params': params}, x)\n",
        "        preds = jnp.clip(preds, 1e-7, 1 - 1e-7)\n",
        "        return -jnp.mean(y * jnp.log(preds) + (1 - y) * jnp.log(1 - preds))\n",
        "    \n",
        "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "bf6ac8bd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss 0.4823\n",
            "Epoch 2, Loss 0.4088\n",
            "Epoch 3, Loss 0.3963\n",
            "Epoch 4, Loss 0.3908\n",
            "Epoch 5, Loss 0.3881\n",
            "Epoch 6, Loss 0.3860\n",
            "Epoch 7, Loss 0.3849\n",
            "Epoch 8, Loss 0.3838\n",
            "Epoch 9, Loss 0.3832\n",
            "Epoch 10, Loss 0.3824\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "n_epochs = 10\n",
        "rng = random.PRNGKey(0)\n",
        "train_labels = train_labels.astype(jnp.float32)\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    rng, input_rng = random.split(rng)\n",
        "    losses = []\n",
        "    for x_batch, y_batch in data_loader(train_features, train_labels, batch_size, input_rng):\n",
        "        state, loss = train_step(state, x_batch, y_batch)\n",
        "        losses.append(loss)\n",
        "    print(f'Epoch {epoch}, Loss {np.mean(jax.device_get(losses)):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "5afdd6e5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: 0.8073394495412844\n"
          ]
        }
      ],
      "source": [
        "probs = state.apply_fn({'params': state.params}, dev_features)\n",
        "\n",
        "pred_labels = (probs > 0.5).astype(jnp.int32)\n",
        "\n",
        "accuracy = accuracy_score(np.array(dev_labels), np.array(pred_labels))\n",
        "print('accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RUbjivUTejxn",
      "metadata": {
        "id": "RUbjivUTejxn"
      },
      "source": [
        "## 77. GPU上での学習\n",
        "\n",
        "問題76のモデル学習をGPU上で実行せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28d542a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 76で実施済み"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZUY1PsD-eplq",
      "metadata": {
        "id": "ZUY1PsD-eplq"
      },
      "source": [
        "## 78. 単語埋め込みのファインチューニング\n",
        "\n",
        "問題77の学習において、単語埋め込みのパラメータも同時に更新するファインチューニングを導入せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d4f88b03",
      "metadata": {},
      "outputs": [],
      "source": [
        "import flax.linen as nn\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "init_embed = nn.initializers.constant(np_embedding_matrix)\n",
        "\n",
        "class FineTuneModel(nn.Module):\n",
        "    vocab_size: int\n",
        "    embed_dim: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x_emb = nn.Embed(\n",
        "            num_embeddings=self.vocab_size,\n",
        "            features=self.embed_dim,\n",
        "            embedding_init=init_embed\n",
        "        )(x)\n",
        "    \n",
        "        x_feat = jnp.mean(x_emb, axis=1)\n",
        "\n",
        "        logits = nn.Dense(features=1)(x_feat)\n",
        "        return jax.nn.sigmoid(logits.squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "97737c3d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available devices: [CudaDevice(id=0), CudaDevice(id=1)]\n",
            "Using device: cuda:1\n"
          ]
        }
      ],
      "source": [
        "print('Available devices:', jax.devices())\n",
        "\n",
        "gpu_id = 1\n",
        "device = jax.devices('gpu')[gpu_id]\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fd0144eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from flax.training import train_state\n",
        "import optax\n",
        "\n",
        "class TrainState(train_state.TrainState):\n",
        "    pass\n",
        "\n",
        "model = FineTuneModel(vocab_size=vocab_size, embed_dim=embedding_dim)\n",
        "key = jax.random.PRNGKey(0)\n",
        "\n",
        "dummy_x = collated_train_data[0]['input_ids'][None, :]\n",
        "params = model.init(key, dummy_x)['params']\n",
        "\n",
        "tx = optax.adam(learning_rate=0.1)\n",
        "state = TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6e9c93a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x = jnp.stack([d['input_ids'] for d in collated_train_data])\n",
        "train_y = jnp.stack([d['label'] for d in collated_train_data]).astype(jnp.float32).squeeze()\n",
        "\n",
        "\n",
        "dev_x = jnp.stack([d['input_ids'] for d in collated_dev_data])\n",
        "dev_y = jnp.stack([d['label'] for d in collated_dev_data]).astype(jnp.float32).squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d7ca67b4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(66650, 270)\n",
            "(66650,)\n"
          ]
        }
      ],
      "source": [
        "print(train_x.shape)\n",
        "print(train_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "81e5ab0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_loader(inputs: jnp.ndarray, labels: jnp.ndarray, batch_size: int, rng):\n",
        "    N = inputs.shape[0]\n",
        "    perm = random.permutation(rng, N)\n",
        "    n_samples = (N // batch_size) * batch_size\n",
        "    perm = perm[:n_samples].reshape(-1, batch_size)\n",
        "    for idx in perm:\n",
        "        yield inputs[idx], labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b6e52456",
      "metadata": {},
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def train_step(state, x_batch, y_batch):\n",
        "    def loss_fn(params):\n",
        "        preds = state.apply_fn({'params': params}, x_batch)\n",
        "        preds = jnp.clip(preds, 1e-7, 1 - 1e-7)\n",
        "        return -jnp.mean(y_batch * jnp.log(preds) + (1-y_batch) * jnp.log(1-preds))\n",
        "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "29ea013d",
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'jax.pmap'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfunctools\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpmap\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpmap\u001b[39;00m\n\u001b[32m      5\u001b[39m n_devices = \u001b[38;5;28mlen\u001b[39m(jax.devices(\u001b[33m'\u001b[39m\u001b[33mgpu\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_devices\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m GPUs\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'jax.pmap'"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "\n",
        "import jax.pmap as pmap\n",
        "\n",
        "n_devices = len(jax.devices('gpu'))\n",
        "print(f\"Training on {n_devices} GPUs\")\n",
        "\n",
        "pmap_keys = jax.random.split(jax.random.PRNGKey(0), n_devices)\n",
        "\n",
        "@functools.partial(pmap, axis_name='devices')\n",
        "def train_step_pmap(state, x_batch, y_batch):\n",
        "    def loss_fn(params):\n",
        "        preds = state.apply_fn({'params': params}, x_batch)\n",
        "        preds = jnp.clip(preds, 1e-7, 1 - 1e-7)\n",
        "        return -jnp.mean(y_batch * jnp.log(preds) + (1-y_batch) * jnp.log(1-preds))\n",
        "    \n",
        "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
        "    # デバイス間で勾配を平均化\n",
        "    grads = jax.lax.pmean(grads, axis_name='devices')\n",
        "    state = state.apply_gradients(grads=grads)\n",
        "    return state, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb8d823c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-19 17:39:47.791763: E external/xla/xla/service/gpu/gpu_hlo_schedule.cc:652] The byte size of input/output arguments (21600015624) exceeds the base limit (15763193856). This indicates an error in the calculation!\n",
            "2025-05-19 17:39:47.796768: W external/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3021] Can't reduce memory use below 0B (0 bytes) by rematerialization; only reduced to 16.76GiB (18000009732 bytes), down from 16.76GiB (18000009732 bytes) originally\n",
            "2025-05-19 17:39:58.219366: W external/xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_1_bfc) ran out of memory trying to allocate 3.35GiB (rounded to 3600001280)requested by op \n",
            "2025-05-19 17:39:58.219725: W external/xla/xla/tsl/framework/bfc_allocator.cc:512] ********************************************************************************************________\n",
            "E0519 17:39:58.219766   61510 pjrt_stream_executor_client.cc:2839] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 3600001200 bytes. [tf-allocator-allocation-error='']\n"
          ]
        },
        {
          "ename": "XlaRuntimeError",
          "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 3600001200 bytes.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mXlaRuntimeError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m gc.collect()\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m data_loader(train_x, train_y, batch_size, key):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     state, loss = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     losses.append(loss)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[finetune] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjnp.mean(jnp.stack(losses))\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "    \u001b[31m[... skipping hidden 5 frame]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/2025/.venv/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:1298\u001b[39m, in \u001b[36mExecuteReplicated.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1296\u001b[39m   \u001b[38;5;28mself\u001b[39m._handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[32m   1297\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1298\u001b[39m   results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mxla_executable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dispatch.needs_check_special():\n\u001b[32m   1301\u001b[39m   out_arrays = results.disassemble_into_single_device_arrays()\n",
            "\u001b[31mXlaRuntimeError\u001b[39m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 3600001200 bytes."
          ]
        }
      ],
      "source": [
        "import jax.random as random\n",
        "import gc\n",
        "batch_size = 1\n",
        "n_epochs = 5\n",
        "rng = random.PRNGKey(0)\n",
        "\n",
        "with jax.default_device(device):\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        rng, key = random.split(rng)\n",
        "        losses = []\n",
        "\n",
        "        gc.collect()\n",
        "        for x_batch, y_batch in data_loader(train_x, train_y, batch_size, key):\n",
        "            state, loss = train_step_pmap(state, x_batch, y_batch)\n",
        "            losses.append(loss)\n",
        "        print(f\"[finetune] Epoch {epoch}, loss = {jnp.mean(jnp.stack(losses)):.4f}\")\n",
        "\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jVAdWIq0evKR",
      "metadata": {
        "id": "jVAdWIq0evKR"
      },
      "source": [
        "## 79. アーキテクチャの変更\n",
        "\n",
        "ニューラルネットワークのアーキテクチャを自由に変更し、モデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。例えば、テキストの特徴ベクトル（単語埋め込みの平均ベクトル）に対して多層のニューラルネットワークを通したり、畳み込みニューラルネットワーク（CNN; Convolutional Neural Network）や再帰型ニューラルネットワーク（RNN; Recurrent Neural Network）などのモデルの学習に挑戦するとよい。"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
